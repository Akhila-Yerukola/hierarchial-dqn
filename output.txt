Annealing factor: 7.5e-05


### EPISODE 0###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.06970024  0.02722208]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.999925
actor_epsilon 3: 0.999925


### EPISODE 1###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.04447432  0.01862596]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.07672306 -0.00086069]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.0411041   0.04005332]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99985
actor_epsilon 4: 0.999925


### EPISODE 2###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.04209072  0.06359638]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.06883943  0.0610522 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.999775
actor_epsilon 4: 0.666666666667
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.06148749  0.07791101]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00101452  0.12463102]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9997
actor_epsilon 4: 0.75


### EPISODE 3###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.01181287  0.1327709 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0215646   0.20829608]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.999625
actor_epsilon 4: 0.8


### EPISODE 4###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.01012937  0.08157554]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.99955
actor_epsilon 1: 0.5


### EPISODE 5###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.01407859  0.09959826]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05690527  0.19326639]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03707034  0.14084099]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.07197474  0.19674383]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.04384626  0.1339978 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.07957064  0.19821982]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.0356489   0.11885102]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.06114784  0.18312065]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.03689859  0.13155554]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.03458053  0.14838454]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.01783272  0.06552461]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.00589738  0.12811482]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.04016287  0.06134588]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01546951  0.21089511]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.06288175  0.05652525]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01140551  0.20410454]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.08880655  0.04468866]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.08372471  0.09882703]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.10746697  0.04646929]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.12236045  0.06574061]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.15109974  0.00213856]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.15973182  0.03523782]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.16844632 -0.00021191]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.13506958  0.14095145]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.15041922  0.002103  ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.14872286 -0.0158381 ]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.12493851 -0.02211872]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.12684484 -0.03240542]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.09191342 -0.01070711]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.10206226 -0.04588966]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.05841562  0.00407928]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.07287393 -0.04326457]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.02020809  0.03832884]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.03547436 -0.01351801]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[  1.00135803e-05   7.09615201e-02]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.00394898  0.01309827]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.05106616  0.0796371 ]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.02395156  0.03781918]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.03591221  0.08859856]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.00332545  0.09053962]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.01038451  0.06592922]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.03771633  0.11397953]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.07711424  0.00896214]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.08457632  0.01894644]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.999475
actor_epsilon 2: 0.5
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.9994
actor_epsilon 2: 0.666666666667


### EPISODE 6###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.999325
actor_epsilon 4: 0.833333333333


### EPISODE 7###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.1534965  -0.04619297]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99925
actor_epsilon 3: 0.99985


### EPISODE 8###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.15793389  0.04268499]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.07871986  0.53989148]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.1410566   0.01716496]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.999175
actor_epsilon 4: 0.857142857143


### EPISODE 9###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.02415082  0.63734245]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9991
actor_epsilon 5: 0.999925


### EPISODE 10###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00510002  0.59623635]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.999025
actor_epsilon 5: 0.99985


### EPISODE 11###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.03363807  0.65533137]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99895
actor_epsilon 6: 0.999925


### EPISODE 12###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.998875
actor_epsilon 2: 0.75


### EPISODE 13###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.21088925  1.08648634]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.24902859  0.92437333]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24134678  0.65036285]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.23450086  0.78871387]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.20681001  0.50674099]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.16782959  0.60980254]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.06675522  0.17025444]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.07749684  0.30476898]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.08968879  0.20979837]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.06968885  0.25239277]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.06634833  0.05691415]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.04562636  0.22594747]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00920281  0.02513412]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.00394922  0.1436995 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03789127 -0.09124139]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05719889 -0.02330765]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9988
actor_epsilon 1: 0.333333333333


### EPISODE 14###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.998725
actor_epsilon 1: 0.25


### EPISODE 15###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.13346258  0.05743912]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99865
actor_epsilon 2: 0.8


### EPISODE 16###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.2026082   0.05766523]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.998575
actor_epsilon 5: 0.999775


### EPISODE 17###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.24239342  0.09767261]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9985
actor_epsilon 5: 0.9997


### EPISODE 18###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.26201183  0.23899384]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.998425
actor_epsilon 3: 0.999775


### EPISODE 19###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.27980798  0.11561306]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99835
actor_epsilon 6: 0.99985


### EPISODE 20###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.27641386  0.11553168]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.998275
actor_epsilon 2: 0.833333333333


### EPISODE 21###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.35965455  0.10111919]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9982
actor_epsilon 4: 0.875


### EPISODE 22###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.25067696  0.13232997]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.998125
actor_epsilon 3: 0.9997


### EPISODE 23###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.2153496   0.13891524]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99805
actor_epsilon 3: 0.999625


### EPISODE 24###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.20115456  0.09227285]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.997975
actor_epsilon 5: 0.999625


### EPISODE 25###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.16814648  0.12361127]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9979
actor_epsilon 5: 0.99955


### EPISODE 26###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13499561  0.15471545]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11545552  0.11774141]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06139544  0.20376635]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.997825
actor_epsilon 5: 0.999475


### EPISODE 27###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.01781328  0.19285774]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.02958165  0.10680938]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.11090893  0.0041396 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.99775
actor_epsilon 5: 0.888888888889
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.08752239  0.01336259]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.03709759  0.09514582]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.997675
actor_epsilon 5: 0.8
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.06899018 -0.02201664]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01065104  0.05110544]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9976
actor_epsilon 2: 0.714285714286
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.17680675  0.20101896]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.23498037  0.12250215]]
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.997525
actor_epsilon 5: 0.818181818182


### EPISODE 28###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.26453418  0.0896374 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99745
actor_epsilon 5: 0.833333333333


### EPISODE 29###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.20195448  0.01665103]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.997375
actor_epsilon 2: 0.75


### EPISODE 30###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9973
actor_epsilon 4: 0.888888888889


### EPISODE 31###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.997225
actor_epsilon 1: 0.2


### EPISODE 32###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.99715
actor_epsilon 1: 0.166666666667


### EPISODE 33###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.997075
actor_epsilon 2: 0.777777777778


### EPISODE 34###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.09929832  0.08712584]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.997
actor_epsilon 4: 0.9


### EPISODE 35###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.14918631  0.17708814]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07013611  0.16480471]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08207167  0.22162259]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00349873  0.16587265]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08275703  0.21974868]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02030789  0.12202042]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.13019165  0.17861393]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996925
actor_epsilon 5: 0.846153846154


### EPISODE 36###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99685
actor_epsilon 2: 0.8


### EPISODE 37###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996775
actor_epsilon 4: 0.909090909091


### EPISODE 38###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.22126359  0.02192658]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9967
actor_epsilon 6: 0.999775


### EPISODE 39###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.24784945  0.1175755 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996625
actor_epsilon 5: 0.857142857143


### EPISODE 40###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.07725577 -0.0213528 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99655
actor_epsilon 4: 0.916666666667


### EPISODE 41###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.31931493  0.05833739]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996475
actor_epsilon 6: 0.9997


### EPISODE 42###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.34841728  0.08327135]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9964
actor_epsilon 6: 0.999625


### EPISODE 43###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.1273555   0.03494111]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996325
actor_epsilon 4: 0.923076923077


### EPISODE 44###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.99625
actor_epsilon 1: 0.142857142857


### EPISODE 45###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.34031853  0.22375882]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996175
actor_epsilon 5: 0.866666666667


### EPISODE 46###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9961
actor_epsilon 1: 0.125


### EPISODE 47###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.04643548  0.05088481]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.996025
actor_epsilon 3: 0.99955


### EPISODE 48###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99595
actor_epsilon 2: 0.818181818182


### EPISODE 49###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.995875
actor_epsilon 1: 0.111111111111


### EPISODE 50###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0713978   0.12526372]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.03145198  0.08505422]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.11537382  0.1600771 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9958
actor_epsilon 5: 0.875


### EPISODE 51###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.20713703  0.03443483]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.995725
actor_epsilon 6: 0.99955


### EPISODE 52###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.00326465  0.07939574]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99565
actor_epsilon 2: 0.833333333333


### EPISODE 53###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.02518267  0.06343868]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0473637   0.19447269]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.995575
actor_epsilon 2: 0.769230769231
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07210533  0.06378531]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9955
actor_epsilon 3: 0.999475


### EPISODE 54###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.995425
actor_epsilon 5: 0.882352941176


### EPISODE 55###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.29947677  0.08631274]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99535
actor_epsilon 5: 0.888888888889


### EPISODE 56###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.13885202 -0.002204  ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.995275
actor_epsilon 4: 0.928571428571


### EPISODE 57###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12543835  0.02699152]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9952
actor_epsilon 3: 0.9994


### EPISODE 58###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.995125
actor_epsilon 2: 0.785714285714


### EPISODE 59###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07217275  0.06532639]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99505
actor_epsilon 3: 0.999325


### EPISODE 60###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.12166196  0.19379339]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.994975
actor_epsilon 2: 0.733333333333
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9949
actor_epsilon 1: 0.1


### EPISODE 61###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.05578525  0.09161451]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.994825
actor_epsilon 4: 0.933333333333


### EPISODE 62###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07170673  0.08292395]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.08680722  0.06919283]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.04317389  0.01447785]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.16343652 -0.05275464]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01120334  0.01734787]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01463392  0.09799317]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.2398324   0.03699097]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99475
actor_epsilon 6: 0.999475


### EPISODE 63###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.994675
actor_epsilon 1: 0.1


### EPISODE 64###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.10193131  0.11037311]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9946
actor_epsilon 4: 0.9375


### EPISODE 65###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.994525
actor_epsilon 1: 0.1


### EPISODE 66###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.13091499 -0.08886388]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99445
actor_epsilon 6: 0.9994


### EPISODE 67###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.10309339  0.08758557]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.994375
actor_epsilon 3: 0.99925


### EPISODE 68###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.08539669  0.07899374]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9943
actor_epsilon 4: 0.941176470588


### EPISODE 69###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.17928329  0.10582441]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.994225
actor_epsilon 3: 0.916666666667
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.03914777  0.40064552]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.99415
actor_epsilon 2: 0.6875
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.994075
actor_epsilon 1: 0.1


### EPISODE 70###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.18555717  0.04950145]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.994
actor_epsilon 5: 0.894736842105


### EPISODE 71###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.17400889  0.40635967]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.14710727 -0.04634461]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.1848221  -0.08715075]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.993925
actor_epsilon 2: 0.647058823529
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.06283789 -0.08314756]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99385
actor_epsilon 2: 0.666666666667


### EPISODE 72###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.16939618  0.05478272]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.26102102  0.04194438]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.30577961  0.1429967 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.25857043  0.05471233]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.22418621  0.05361712]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.15268692  0.02458045]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.07042276 -0.05369759]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.993775
actor_epsilon 6: 0.999325


### EPISODE 73###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.04114946 -0.15211704]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9937
actor_epsilon 5: 0.9


### EPISODE 74###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.03122793 -0.14004198]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.993625
actor_epsilon 5: 0.904761904762


### EPISODE 75###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.22592433  0.08186796]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99355
actor_epsilon 3: 0.923076923077


### EPISODE 76###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.993475
actor_epsilon 2: 0.684210526316


### EPISODE 77###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9934
actor_epsilon 1: 0.1


### EPISODE 78###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.19573973 -0.03370166]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.993325
actor_epsilon 5: 0.909090909091


### EPISODE 79###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.02457097  0.01591966]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99325
actor_epsilon 4: 0.944444444444


### EPISODE 80###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.18569577 -0.0895505 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.07677695  0.13198587]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.06972179 -0.16409436]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.993175
actor_epsilon 5: 0.913043478261


### EPISODE 81###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.10732464  0.05458647]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9931
actor_epsilon 2: 0.7


### EPISODE 82###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.08982407  0.05887204]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.993025
actor_epsilon 3: 0.928571428571


### EPISODE 83###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.13388173  0.06744978]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99295
actor_epsilon 2: 0.714285714286


### EPISODE 84###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.12710193  0.07454297]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.992875
actor_epsilon 2: 0.727272727273


### EPISODE 85###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.04591377  0.13463716]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9928
actor_epsilon 5: 0.916666666667


### EPISODE 86###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00114685  0.18011534]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.05031852  1.30583143]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.24003215  0.01056647]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.04188515 -0.06367382]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.992725
actor_epsilon 5: 0.92


### EPISODE 87###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.10585377 -0.03981328]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99265
actor_epsilon 2: 0.739130434783


### EPISODE 88###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.15558061 -0.24439344]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.992575
actor_epsilon 6: 0.99925


### EPISODE 89###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.16453981 -0.14169583]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9925
actor_epsilon 6: 0.999175


### EPISODE 90###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11351039  0.56225646]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.06536582  0.05414608]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.28835577  0.49526912]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.992425
actor_epsilon 2: 0.708333333333
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.04880581  0.09263134]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.99235
actor_epsilon 3: 0.866666666667
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.08894466  0.00870553]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.992275
actor_epsilon 5: 0.923076923077


### EPISODE 91###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.04938523  0.11593297]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9922
actor_epsilon 3: 0.8125
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.62984687  0.38378745]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.0477959   0.07333285]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.992125
actor_epsilon 4: 0.947368421053


### EPISODE 92###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.62536091  0.46990672]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.07954096  0.22438999]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99205
actor_epsilon 4: 0.95


### EPISODE 93###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.03591476 -0.03915626]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.991975
actor_epsilon 5: 0.925925925926


### EPISODE 94###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.14021581  0.19675265]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.34292498  0.03211421]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9919
actor_epsilon 2: 0.68
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.02928267  0.16213554]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.991825
actor_epsilon 3: 0.764705882353
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.49793428  0.20147946]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.14218801  0.197496  ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.31925255  0.36374602]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.99175
actor_epsilon 4: 0.904761904762
Exploring

New Goal: 3
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.991675
actor_epsilon 3: 0.722222222222
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.39621291  0.27401078]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.25372207  0.04454345]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.39281756  0.19786337]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.20239541 -0.02067691]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.33153313  0.15288106]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.23197764  0.05587098]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9916
actor_epsilon 6: 0.9991


### EPISODE 95###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.21102579  0.09120506]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.991525
actor_epsilon 6: 0.999025


### EPISODE 96###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.0336099   0.11532055]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.21097875  0.61521924]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07601266  0.11667925]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.99145
actor_epsilon 2: 0.653846153846
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.38087499  0.55503392]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.991375
actor_epsilon 2: 0.62962962963
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.0657445  -0.02838993]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9913
actor_epsilon 4: 0.909090909091


### EPISODE 97###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.991225
actor_epsilon 1: 0.1


### EPISODE 98###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.06582536  0.34837982]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.99115
actor_epsilon 3: 0.684210526316
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.49749622  0.52955109]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.991075
actor_epsilon 4: 0.869565217391
Exploring

New Goal: 1
State-Actions: 
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.991
actor_epsilon 1: 0.1


### EPISODE 99###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.08226786  0.11281307]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.14867109  0.00064805]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.04712152  0.46860564]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.990925
actor_epsilon 5: 0.892857142857
Exploring

New Goal: 2
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01332478  0.15093617]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.99085
actor_epsilon 2: 0.607142857143
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0326857   0.08097515]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.24754363  0.21630616]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05838889  0.18781771]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.24176808  0.27655464]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08011508  0.21762869]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.990775
actor_epsilon 6: 0.99895

[[ 0.1    0.143  0.067  0.037  0.023  0.01 ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 100###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.11536549  0.13249956]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.16919717  0.15446323]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.06476283 -0.0570455 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.16933662  0.08130834]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.12089717 -0.12908278]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.21607047  0.07434435]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.03065482 -0.15353377]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9907
actor_epsilon 6: 0.998875


### EPISODE 101###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.04426141  0.12773746]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.990625
actor_epsilon 4: 0.875


### EPISODE 102###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.10214617 -0.15201183]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99055
actor_epsilon 6: 0.9988


### EPISODE 103###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.990475
actor_epsilon 4: 0.88


### EPISODE 104###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.09590784 -0.24387601]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9904
actor_epsilon 6: 0.998725


### EPISODE 105###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.09255946  0.06098609]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.990325
actor_epsilon 2: 0.620689655172


### EPISODE 106###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.99025
actor_epsilon 2: 0.633333333333


### EPISODE 107###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.990175
actor_epsilon 1: 0.1


### EPISODE 108###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.14911845  0.43722248]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9901
actor_epsilon 3: 0.65
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.57834619  0.61373788]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.990025
actor_epsilon 2: 0.612903225806
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98995
actor_epsilon 1: 0.1


### EPISODE 109###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.29572642  0.22295803]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.989875
actor_epsilon 4: 0.884615384615


### EPISODE 110###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.23417673  0.07666916]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9898
actor_epsilon 2: 0.625


### EPISODE 111###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.989725
actor_epsilon 5: 0.896551724138


### EPISODE 112###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.06873477 -0.21059689]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98965
actor_epsilon 6: 0.99865


### EPISODE 113###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.03103465 -0.2212027 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.989575
actor_epsilon 6: 0.998575


### EPISODE 114###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.03082383  0.02151445]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9895
actor_epsilon 5: 0.9


### EPISODE 115###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.06480019 -0.15259534]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.989425
actor_epsilon 6: 0.9985


### EPISODE 116###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.11555146 -0.10963865]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.29396439  0.12168513]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.32172382  0.09214091]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.19495182  0.07729894]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.98935
actor_epsilon 6: 0.954545454545
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.26177078  0.36528099]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.09290868 -0.00175324]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.989275
actor_epsilon 6: 0.913043478261
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.71519446  1.02041698]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9892
actor_epsilon 5: 0.870967741935
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.23710126  0.22422281]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.38535786  0.59393412]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.989125
actor_epsilon 5: 0.84375
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0218071   0.20206869]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.15477936  0.25029486]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.16698161  0.11004114]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08125967  0.07519304]]
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.98905
actor_epsilon 6: 0.916666666667


### EPISODE 117###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.08176693  0.08203401]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988975
actor_epsilon 5: 0.848484848485


### EPISODE 118###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9889
actor_epsilon 1: 0.1


### EPISODE 119###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.05567101 -0.00613979]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988825
actor_epsilon 6: 0.92


### EPISODE 120###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98875
actor_epsilon 1: 0.1


### EPISODE 121###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00273651  0.11208017]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988675
actor_epsilon 5: 0.852941176471


### EPISODE 122###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0085007   0.11348616]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0680466  -0.08722638]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.03738427  0.02210143]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9886
actor_epsilon 5: 0.857142857143


### EPISODE 123###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.07126603 -0.05951625]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988525
actor_epsilon 5: 0.861111111111


### EPISODE 124###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.11346103 -0.15125026]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98845
actor_epsilon 2: 0.636363636364


### EPISODE 125###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.11752963  0.39949402]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.988375
actor_epsilon 1: 0.1


### EPISODE 126###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.11717072  0.05015111]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9883
actor_epsilon 6: 0.923076923077


### EPISODE 127###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988225
actor_epsilon 3: 0.666666666667


### EPISODE 128###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98815
actor_epsilon 3: 0.681818181818


### EPISODE 129###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.11475006  0.00872383]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988075
actor_epsilon 2: 0.647058823529


### EPISODE 130###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.17185865  0.08991518]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.988
actor_epsilon 4: 0.888888888889


### EPISODE 131###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.987925
actor_epsilon 2: 0.657142857143


### EPISODE 132###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07644805  0.46582544]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.98785
actor_epsilon 3: 0.652173913043
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.1728517   0.02437928]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.00830993  0.06397523]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.16454062  0.01123562]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.07622576  0.08337636]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.987775
actor_epsilon 6: 0.925925925926


### EPISODE 133###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.10188285  0.1401578 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9877
actor_epsilon 6: 0.928571428571


### EPISODE 134###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.987625
actor_epsilon 1: 0.1


### EPISODE 135###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.14258453  0.12142422]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98755
actor_epsilon 6: 0.931034482759


### EPISODE 136###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.12534592  0.10689157]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.34610346  0.04408165]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.13157377  0.07635619]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.34809968  0.0778278 ]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.987475
actor_epsilon 4: 0.892857142857


### EPISODE 137###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9874
actor_epsilon 1: 0.1


### EPISODE 138###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06358692  0.05972546]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.987325
actor_epsilon 5: 0.864864864865


### EPISODE 139###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98725
actor_epsilon 1: 0.1


### EPISODE 140###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.16103324 -0.02536378]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.987175
actor_epsilon 6: 0.933333333333


### EPISODE 141###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06609061  0.00142522]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9871
actor_epsilon 5: 0.868421052632


### EPISODE 142###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.88571984  0.32125857]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.987025
actor_epsilon 1: 0.1


### EPISODE 143###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.2628575   0.20554537]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1332458   0.18379785]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.24710123  0.1841937 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98695
actor_epsilon 4: 0.896551724138


### EPISODE 144###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.986875
actor_epsilon 4: 0.9


### EPISODE 145###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.03983727  0.09078125]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9868
actor_epsilon 2: 0.666666666667


### EPISODE 146###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.02953598  0.48602769]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.986725
actor_epsilon 3: 0.666666666667


### EPISODE 147###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06536609 -0.01359937]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98665
actor_epsilon 5: 0.871794871795


### EPISODE 148###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.08545706  0.02749618]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.986575
actor_epsilon 2: 0.675675675676


### EPISODE 149###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.30369401  0.09960695]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07130802  0.22451854]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.35044158  0.11234839]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9865
actor_epsilon 4: 0.903225806452


### EPISODE 150###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.09054103  0.02770261]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.986425
actor_epsilon 5: 0.875


### EPISODE 151###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98635
actor_epsilon 2: 0.684210526316


### EPISODE 152###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.0852606   0.67654049]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.986275
actor_epsilon 3: 0.68


### EPISODE 153###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.13732576  0.93580413]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9862
actor_epsilon 2: 0.666666666667
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.986125
actor_epsilon 1: 0.1


### EPISODE 154###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.02344096 -0.04811078]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98605
actor_epsilon 2: 0.675


### EPISODE 155###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08712843 -0.07292986]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.985975
actor_epsilon 6: 0.935483870968


### EPISODE 156###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.01754591 -0.07838146]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9859
actor_epsilon 2: 0.682926829268


### EPISODE 157###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.985825
actor_epsilon 1: 0.1


### EPISODE 158###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.05793735 -0.00777957]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98575
actor_epsilon 6: 0.9375


### EPISODE 159###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.985675
actor_epsilon 1: 0.1


### EPISODE 160###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.02881083  0.02474189]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9856
actor_epsilon 2: 0.666666666667
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.985525
actor_epsilon 2: 0.674418604651


### EPISODE 161###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98545
actor_epsilon 1: 0.1


### EPISODE 162###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.13169399 -0.04641131]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.985375
actor_epsilon 2: 0.681818181818


### EPISODE 163###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9853
actor_epsilon 1: 0.1


### EPISODE 164###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.20892474  0.41465735]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.985225
actor_epsilon 3: 0.653846153846
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.17835045  0.01397761]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14189237  0.13888174]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98515
actor_epsilon 6: 0.939393939394


### EPISODE 165###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.3730832   0.58802813]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.985075
actor_epsilon 3: 0.666666666667


### EPISODE 166###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.985
actor_epsilon 5: 0.878048780488


### EPISODE 167###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.04532388  0.22782527]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.984925
actor_epsilon 6: 0.941176470588


### EPISODE 168###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.41713575  0.64610988]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.98485
actor_epsilon 3: 0.642857142857
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12525433  0.04993345]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16698733 -0.040618  ]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.984775
actor_epsilon 1: 0.1


### EPISODE 169###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00339159  0.12365515]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9847
actor_epsilon 6: 0.942857142857


### EPISODE 170###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00739729  0.15351462]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0538936   0.09554812]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.02961671  0.15522633]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.01582694  0.16551219]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.984625
actor_epsilon 6: 0.916666666667
Exploring

New Goal: 4
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.2665711  -0.06780424]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.10999736 -0.04161608]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.31331652 -0.10243765]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.98455
actor_epsilon 4: 0.875
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.28059754  0.0486348 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.3126753  -0.07612949]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.984475
actor_epsilon 4: 0.848484848485
Exploring

New Goal: 1
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.11766142 -0.02917457]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.33301574 -0.24376214]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.9844
actor_epsilon 1: 0.1


### EPISODE 171###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.984325
actor_epsilon 3: 0.655172413793


### EPISODE 172###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98425
actor_epsilon 1: 0.1


### EPISODE 173###

New Goal: 2
State-Actions: 
Here ------> [[ 0.1008451  0.0593073]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.984175
actor_epsilon 2: 0.688888888889


### EPISODE 174###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.73301303  0.3802008 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9841
actor_epsilon 1: 0.1


### EPISODE 175###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.984025
actor_epsilon 1: 0.1


### EPISODE 176###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.19368279  0.05505398]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98395
actor_epsilon 4: 0.852941176471


### EPISODE 177###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.983875
actor_epsilon 6: 0.918918918919


### EPISODE 178###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9838
actor_epsilon 2: 0.695652173913


### EPISODE 179###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.983725
actor_epsilon 1: 0.1


### EPISODE 180###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98365
actor_epsilon 3: 0.666666666667


### EPISODE 181###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.10963896  0.07906412]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.11456916  0.06151945]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.983575
actor_epsilon 5: 0.880952380952


### EPISODE 182###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.11100018  0.25211561]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9835
actor_epsilon 3: 0.645161290323
Exploring

New Goal: 2
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.15815219 -0.05415066]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.67376262  0.40378916]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.983425
actor_epsilon 2: 0.68085106383
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.06258774  0.09568578]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98335
actor_epsilon 6: 0.921052631579


### EPISODE 183###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.983275
actor_epsilon 1: 0.1


### EPISODE 184###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.15309796 -0.02388323]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9832
actor_epsilon 5: 0.883720930233


### EPISODE 185###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.04761335  0.06103182]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.08364508 -0.07588026]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.06933996  0.07462613]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.07076883 -0.08031845]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.02833965  0.03641123]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.02023229  0.04025063]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.01334137  0.05806685]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.983125
actor_epsilon 6: 0.923076923077


### EPISODE 186###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.09194982 -0.01800399]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98305
actor_epsilon 2: 0.6875


### EPISODE 187###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.982975
actor_epsilon 1: 0.1


### EPISODE 188###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9829
actor_epsilon 3: 0.625
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.03402472  0.04117285]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.06756574  0.13198167]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.15616331  0.17579786]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03955874  0.1485834 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.12314889  0.19497526]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01441783  0.16444138]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.07301909  0.12521438]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.06104484  0.19227956]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.982825
actor_epsilon 6: 0.9
Exploring

New Goal: 1
State-Actions: 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.98275
actor_epsilon 1: 0.1


### EPISODE 189###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.982675
actor_epsilon 3: 0.606060606061
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.02467257  0.0586656 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 3.30208826  1.05037057]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.04189107  0.03884163]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 3.2799325   0.99386787]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.05585274  0.020297  ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 3.11319137  0.90658706]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9826
actor_epsilon 5: 0.886363636364


### EPISODE 190###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.15417561 -0.01912642]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.982525
actor_epsilon 5: 0.888888888889


### EPISODE 191###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.33595905  0.33391583]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98245
actor_epsilon 3: 0.617647058824


### EPISODE 192###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.33230323  0.35907641]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.982375
actor_epsilon 3: 0.628571428571


### EPISODE 193###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.32845145  0.38444489]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9823
actor_epsilon 3: 0.638888888889


### EPISODE 194###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.17845944 -0.00912191]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.13131949  0.00356638]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.12706304  0.01011206]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.08785236  0.023102  ]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.12143633  0.02228781]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.08753781  0.03512954]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.982225
actor_epsilon 6: 0.878048780488
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.02668571  0.06832368]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.08774896  0.04600177]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.98215
actor_epsilon 6: 0.857142857143
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.02997398  0.08337033]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.08816123  0.0558913 ]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.10072224  0.05676853]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.16154584  0.04512307]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.982075
actor_epsilon 6: 0.860465116279


### EPISODE 195###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.07047902  0.12812819]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.982
actor_epsilon 2: 0.69387755102


### EPISODE 196###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.10901837  0.07014678]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00841872  0.11079426]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.00248133  0.12415096]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.38369215  0.22764753]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.10368336  0.0790152 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.981925
actor_epsilon 5: 0.891304347826


### EPISODE 197###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98185
actor_epsilon 2: 0.7


### EPISODE 198###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.14834894  0.03058095]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.03541936  0.08760875]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01727277  0.09091439]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.981775
actor_epsilon 1: 0.1


### EPISODE 199###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9817
actor_epsilon 1: 0.1

[[ 0.2    0.28   0.125  0.07   0.046  0.021]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 200###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.04316081  0.05740782]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.981625
actor_epsilon 4: 0.857142857143


### EPISODE 201###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.12185133  0.0117811 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.05391172  0.03919366]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.11825524  0.00113937]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.04414989  0.02510446]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.04546258  0.02186017]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.10256177 -0.00984913]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.02220543  0.02357833]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.09443418 -0.01383211]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98155
actor_epsilon 5: 0.893617021277


### EPISODE 202###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.15526094 -0.047441  ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.981475
actor_epsilon 6: 0.863636363636


### EPISODE 203###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9814
actor_epsilon 3: 0.648648648649


### EPISODE 204###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.04081936  0.05527103]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.981325
actor_epsilon 3: 0.631578947368
Exploring

New Goal: 3
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.97327995  1.37558413]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.98125
actor_epsilon 3: 0.615384615385
Exploring

New Goal: 2
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.04125345  0.62478042]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.981175
actor_epsilon 2: 0.686274509804
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.05764295  0.01847711]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9811
actor_epsilon 5: 0.895833333333


### EPISODE 205###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.981025
actor_epsilon 1: 0.1


### EPISODE 206###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98095
actor_epsilon 1: 0.1


### EPISODE 207###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.980875
actor_epsilon 1: 0.1


### EPISODE 208###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.08330138  0.00843948]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9808
actor_epsilon 4: 0.861111111111


### EPISODE 209###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.05183911  0.07962122]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.980725
actor_epsilon 3: 0.6
Exploring

New Goal: 3
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.04797575  0.07753687]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98065
actor_epsilon 3: 0.609756097561


### EPISODE 210###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.980575
actor_epsilon 2: 0.692307692308


### EPISODE 211###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9805
actor_epsilon 4: 0.864864864865


### EPISODE 212###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.02257691  0.0446473 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.980425
actor_epsilon 2: 0.679245283019
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.71002549  0.20439626]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.98035
actor_epsilon 1: 0.1


### EPISODE 213###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.03803605  0.07300459]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.980275
actor_epsilon 3: 0.595238095238
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9802
actor_epsilon 1: 0.1


### EPISODE 214###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.980125
actor_epsilon 3: 0.604651162791


### EPISODE 215###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.07620431  0.03161442]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12748614  0.11893845]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.06987979  0.03793894]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.98005
actor_epsilon 4: 0.868421052632


### EPISODE 216###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.979975
actor_epsilon 2: 0.685185185185


### EPISODE 217###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.06355526  0.04426346]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.13952085  0.14080909]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.05723072  0.05058799]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9799
actor_epsilon 4: 0.871794871795


### EPISODE 218###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.87616205  0.39037755]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.979825
actor_epsilon 1: 0.1


### EPISODE 219###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.03827266  0.06320261]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97975
actor_epsilon 2: 0.690909090909


### EPISODE 220###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.979675
actor_epsilon 1: 0.1


### EPISODE 221###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.04458164  0.06323704]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9796
actor_epsilon 6: 0.866666666667


### EPISODE 222###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.09638426  0.13878775]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.979525
actor_epsilon 3: 0.613636363636


### EPISODE 223###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.03825711  0.06956157]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97945
actor_epsilon 4: 0.875


### EPISODE 224###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.03509484  0.07272384]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.979375
actor_epsilon 6: 0.869565217391


### EPISODE 225###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.03193257  0.0758861 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18819581  0.20000923]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.02560803  0.08221062]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9793
actor_epsilon 4: 0.878048780488


### EPISODE 226###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.979225
actor_epsilon 1: 0.1


### EPISODE 227###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.0192835   0.08853514]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97915
actor_epsilon 6: 0.872340425532


### EPISODE 228###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01612123  0.09169739]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07356273  0.14608935]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00979669  0.0980219 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.979075
actor_epsilon 5: 0.897959183673


### EPISODE 229###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.979
actor_epsilon 1: 0.1


### EPISODE 230###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.978925
actor_epsilon 1: 0.1


### EPISODE 231###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.00601464  0.11383312]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97885
actor_epsilon 2: 0.696428571429


### EPISODE 232###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.978775
actor_epsilon 2: 0.701754385965


### EPISODE 233###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.01233918  0.12015756]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12746389  0.16762216]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.01866371  0.12648191]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9787
actor_epsilon 4: 0.880952380952


### EPISODE 234###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.02182598  0.12964396]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.978625
actor_epsilon 4: 0.883720930233


### EPISODE 235###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.02498824  0.13280532]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97855
actor_epsilon 5: 0.9


### EPISODE 236###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.978475
actor_epsilon 3: 0.622222222222


### EPISODE 237###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9784
actor_epsilon 1: 0.1


### EPISODE 238###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.03447504  0.12331916]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.978325
actor_epsilon 2: 0.689655172414
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97825
actor_epsilon 2: 0.694915254237


### EPISODE 239###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.04396183  0.11383265]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.978175
actor_epsilon 2: 0.683333333333
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.82341331  0.28707784]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9781
actor_epsilon 1: 0.1


### EPISODE 240###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.978025
actor_epsilon 5: 0.901960784314


### EPISODE 241###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97795
actor_epsilon 1: 0.1


### EPISODE 242###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.977875
actor_epsilon 5: 0.903846153846


### EPISODE 243###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9778
actor_epsilon 3: 0.608695652174
Exploring

New Goal: 1
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.977725
actor_epsilon 1: 0.1


### EPISODE 244###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.04255243  0.11691725]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07558449  0.08221087]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07874675  0.0790488 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08190902  0.07588689]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05477742  0.10460137]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97765
actor_epsilon 6: 0.875


### EPISODE 245###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.977575
actor_epsilon 6: 0.877551020408


### EPISODE 246###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0839593   0.07421433]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9775
actor_epsilon 6: 0.88


### EPISODE 247###

New Goal: 2
State-Actions: 
Here ------> [[ 0.09455808  0.0695642 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.977425
actor_epsilon 2: 0.688524590164


### EPISODE 248###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.09772033  0.07272626]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97735
actor_epsilon 4: 0.886363636364


### EPISODE 249###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.10088259  0.07588836]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.977275
actor_epsilon 4: 0.888888888889


### EPISODE 250###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.10404485  0.07905048]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9772
actor_epsilon 6: 0.882352941176


### EPISODE 251###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.1072071   0.08221263]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.977125
actor_epsilon 4: 0.891304347826


### EPISODE 252###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97705
actor_epsilon 4: 0.893617021277


### EPISODE 253###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.976975
actor_epsilon 2: 0.677419354839
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.11985608  0.09486113]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9769
actor_epsilon 2: 0.68253968254


### EPISODE 254###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.1230183   0.09802307]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976825
actor_epsilon 6: 0.884615384615


### EPISODE 255###

New Goal: 2
State-Actions: 
Here ------> [[ 0.1261805   0.10118492]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97675
actor_epsilon 2: 0.6875


### EPISODE 256###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.12934263  0.09804583]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976675
actor_epsilon 6: 0.88679245283


### EPISODE 257###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9766
actor_epsilon 1: 0.1


### EPISODE 258###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.12934226  0.09172218]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976525
actor_epsilon 5: 0.905660377358


### EPISODE 259###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97645
actor_epsilon 1: 0.1


### EPISODE 260###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.12301791  0.0853982 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976375
actor_epsilon 5: 0.907407407407


### EPISODE 261###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.43802682  0.25880328]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9763
actor_epsilon 3: 0.617021276596


### EPISODE 262###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.11669347  0.079074  ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976225
actor_epsilon 6: 0.888888888889


### EPISODE 263###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.11353123  0.07591189]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97615
actor_epsilon 2: 0.692307692308


### EPISODE 264###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.11036899  0.07274975]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976075
actor_epsilon 5: 0.909090909091


### EPISODE 265###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.10720675  0.06958759]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.976
actor_epsilon 5: 0.910714285714


### EPISODE 266###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.975925
actor_epsilon 1: 0.1


### EPISODE 267###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97585
actor_epsilon 1: 0.1


### EPISODE 268###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.34673747  0.23096758]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.975775
actor_epsilon 3: 0.625


### EPISODE 269###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9757
actor_epsilon 2: 0.69696969697


### EPISODE 270###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.975625
actor_epsilon 1: 0.1


### EPISODE 271###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08823325  0.05061485]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97555
actor_epsilon 6: 0.890909090909


### EPISODE 272###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.085071    0.04745284]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.975475
actor_epsilon 6: 0.892857142857


### EPISODE 273###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.29433358  0.21302417]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9754
actor_epsilon 3: 0.632653061224


### EPISODE 274###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.975325
actor_epsilon 1: 0.1


### EPISODE 275###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.78034163  0.52438188]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97525
actor_epsilon 1: 0.1


### EPISODE 276###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.26668453  0.222028  ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.975175
actor_epsilon 3: 0.64


### EPISODE 277###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06925972  0.05693088]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9751
actor_epsilon 5: 0.912280701754


### EPISODE 278###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.24863285  0.22752061]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.975025
actor_epsilon 3: 0.647058823529


### EPISODE 279###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97495
actor_epsilon 1: 0.1


### EPISODE 280###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.974875
actor_epsilon 3: 0.653846153846


### EPISODE 281###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.05661069  0.05691607]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05344843  0.05375423]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9748
actor_epsilon 4: 0.875
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05028617  0.0505924 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.974725
actor_epsilon 5: 0.896551724138
Exploring

New Goal: 3
State-Actions: 
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.23590527 -0.04296109]]
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.98820472  1.2809571 ]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.70791292  0.83364701]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.97465
actor_epsilon 3: 0.641509433962
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.03447488  0.03478211]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03131262  0.03162007]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02815036  0.02845796]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.0249881   0.02529584]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.974575
actor_epsilon 5: 0.881355932203
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.86759615  1.24173653]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.57579941  0.80652326]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9745
actor_epsilon 3: 0.62962962963
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.4976562   0.71780401]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.42136273  0.652776  ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.00601453  0.01897101]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.974425
actor_epsilon 2: 0.686567164179
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.97435
actor_epsilon 3: 0.636363636364


### EPISODE 282###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.974275
actor_epsilon 6: 0.894736842105


### EPISODE 283###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.00663452  0.03161962]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24748309  0.51263255]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.01295904  0.03794403]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22740006  0.50335199]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9742
actor_epsilon 2: 0.676470588235
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.01928356  0.04426847]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.02244582  0.04743069]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.02560808  0.05059293]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.02877034  0.05375516]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.03509485  0.06007963]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.03825711  0.06324186]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.974125
actor_epsilon 6: 0.896551724138


### EPISODE 284###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.04141937  0.06640409]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97405
actor_epsilon 5: 0.883333333333


### EPISODE 285###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.04458163  0.06956632]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.04774388  0.07272854]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.05090614  0.07589075]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.05406839  0.07905295]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0603929  0.0853773]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.06355515  0.08853944]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0667174   0.09170154]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.06987965  0.09486362]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0793664   0.10434966]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.08252864  0.10751159]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.973975
actor_epsilon 6: 0.898305084746


### EPISODE 286###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.08569089  0.11067338]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.08885312  0.1075137 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.09201535  0.10435197]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.09517757  0.10119016]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.09833978  0.09802819]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.10150197  0.09486616]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9739
actor_epsilon 6: 0.9


### EPISODE 287###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.11952373  0.3332985 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.973825
actor_epsilon 3: 0.642857142857


### EPISODE 288###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97375
actor_epsilon 2: 0.68115942029


### EPISODE 289###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.973675
actor_epsilon 1: 0.1


### EPISODE 290###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.10466371  0.07905547]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.49429265  0.65691543]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.09517714  0.06956907]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.68384385  0.7457543 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.74055088  0.74868256]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.07936598  0.05375823]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.07620373  0.05059606]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.72206211  0.42887601]]
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.06987925  0.04427173]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.75690377  0.44853112]]
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.06355475  0.03794768]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9736
actor_epsilon 2: 0.671428571429
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.15510902  0.16854389]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.973525
actor_epsilon 3: 0.649122807018


### EPISODE 291###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97345
actor_epsilon 6: 0.901639344262


### EPISODE 292###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.0477435   0.03480214]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.973375
actor_epsilon 2: 0.676056338028


### EPISODE 293###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.04458125  0.03796428]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9733
actor_epsilon 5: 0.885245901639


### EPISODE 294###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.16138247  0.16350666]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.973225
actor_epsilon 3: 0.637931034483
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.03825675  0.04428867]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.0350945   0.04745089]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.03193225  0.05061311]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97315
actor_epsilon 6: 0.903225806452


### EPISODE 295###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.973075
actor_epsilon 3: 0.64406779661


### EPISODE 296###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.973
actor_epsilon 1: 0.1


### EPISODE 297###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01928326  0.06326205]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.972925
actor_epsilon 5: 0.887096774194


### EPISODE 298###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97285
actor_epsilon 1: 0.1


### EPISODE 299###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.01295876  0.06958654]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.56568348  0.38799554]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.972775
actor_epsilon 2: 0.666666666667
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.00663426  0.07591104]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.57658947  0.39106125]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9727
actor_epsilon 2: 0.657534246575
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.972625
actor_epsilon 1: 0.1

[[ 0.3    0.41   0.174  0.094  0.054  0.024]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 300###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.2606822   0.21840304]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97255
actor_epsilon 3: 0.65


### EPISODE 301###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.00601474  0.08856007]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.972475
actor_epsilon 2: 0.662162162162


### EPISODE 302###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.009177    0.09172232]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01233925  0.09488458]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9724
actor_epsilon 5: 0.888888888889


### EPISODE 303###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.972325
actor_epsilon 1: 0.1


### EPISODE 304###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.021826    0.10437135]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02498825  0.10753361]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.97225
actor_epsilon 4: 0.857142857143
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0281505   0.11069587]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.972175
actor_epsilon 5: 0.875
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.03131275  0.11385813]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.9721
actor_epsilon 6: 0.888888888889
Exploring

New Goal: 4
State-Actions: 
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.17992733  0.17575826]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.972025
actor_epsilon 4: 0.84
Exploring

New Goal: 1
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.04712392  0.12966943]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.97195
actor_epsilon 1: 0.1


### EPISODE 305###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.971875
actor_epsilon 2: 0.666666666667


### EPISODE 306###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05977303  0.14864294]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05661103  0.15180518]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05344893  0.15496741]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9718
actor_epsilon 5: 0.876923076923


### EPISODE 307###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.971725
actor_epsilon 1: 0.1


### EPISODE 308###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.97165
actor_epsilon 3: 0.639344262295
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.74252075  0.36422738]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.971575
actor_epsilon 2: 0.657894736842
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.7516849   0.37052882]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9715
actor_epsilon 1: 0.1


### EPISODE 309###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.971425
actor_epsilon 3: 0.629032258065
Exploring

New Goal: 5
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.00601657  0.14548257]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97135
actor_epsilon 5: 0.878787878788


### EPISODE 310###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00285497  0.14232036]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.00601575  0.13915816]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.00917762  0.13599595]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.0123396   0.13283375]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01550166  0.12967154]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.01866375  0.12650934]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.971275
actor_epsilon 6: 0.875
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.03569007  0.30983013]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.02498793  0.12018496]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.09133255  0.54266977]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9712
actor_epsilon 2: 0.649350649351
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.03447381  0.1106984 ]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.971125
actor_epsilon 4: 0.843137254902


### EPISODE 311###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97105
actor_epsilon 1: 0.1


### EPISODE 312###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.970975
actor_epsilon 1: 0.1


### EPISODE 313###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.0123374   0.08223899]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9709
actor_epsilon 4: 0.846153846154


### EPISODE 314###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.970825
actor_epsilon 4: 0.849056603774


### EPISODE 315###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.97075
actor_epsilon 1: 0.1


### EPISODE 316###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00285074  0.07275265]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00663592  0.06326723]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00979813  0.06642874]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.970675
actor_epsilon 6: 0.876923076923


### EPISODE 317###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9706
actor_epsilon 1: 0.1


### EPISODE 318###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.01612254  0.07275285]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.970525
actor_epsilon 6: 0.878787878788


### EPISODE 319###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01928473  0.075915  ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.02560904  0.0822394 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0287711   0.08540161]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.03193291  0.08856383]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.97045
actor_epsilon 5: 0.865671641791
Exploring

New Goal: 4
State-Actions: 
Here ------> [[-0.02877184  0.09172605]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.02560998  0.09488828]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.02244793  0.0980505 ]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.970375
actor_epsilon 4: 0.833333333333
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01928583  0.10121274]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9703
actor_epsilon 5: 0.852941176471
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.01612369  0.10437498]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.01296153  0.10753722]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.00979937  0.11069947]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.20031467  0.18461773]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.970225
actor_epsilon 3: 0.619047619048
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00347502  0.11702395]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00031285  0.12018619]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.97015
actor_epsilon 6: 0.880597014925


### EPISODE 320###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.970075
actor_epsilon 6: 0.882352941176


### EPISODE 321###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00601145  0.12651066]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.97
actor_epsilon 6: 0.884057971014


### EPISODE 322###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.44178462  0.29656762]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.969925
actor_epsilon 3: 0.625


### EPISODE 323###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96985
actor_epsilon 5: 0.855072463768


### EPISODE 324###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.969775
actor_epsilon 1: 0.1


### EPISODE 325###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9697
actor_epsilon 2: 0.653846153846


### EPISODE 326###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.00917382  0.14232175]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.969625
actor_epsilon 5: 0.857142857143


### EPISODE 327###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96955
actor_epsilon 6: 0.885714285714


### EPISODE 328###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.51725954  0.30592394]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.969475
actor_epsilon 3: 0.630769230769


### EPISODE 329###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9694
actor_epsilon 1: 0.1


### EPISODE 330###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.00347485  0.15497024]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.95694411  0.42485294]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.969325
actor_epsilon 2: 0.645569620253
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.01295594  0.16736092]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.96925
actor_epsilon 4: 0.818181818182
Exploring

New Goal: 6
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01928592  0.15812677]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.02244814  0.1549646 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.969175
actor_epsilon 6: 0.887323943662


### EPISODE 331###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9691
actor_epsilon 2: 0.65


### EPISODE 332###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.03821747  0.1265046 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.02556867  0.11385563]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.969025
actor_epsilon 1: 0.1


### EPISODE 333###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.01291979  0.10120664]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96895
actor_epsilon 2: 0.654320987654


### EPISODE 334###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.968875
actor_epsilon 2: 0.658536585366


### EPISODE 335###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9688
actor_epsilon 1: 0.1


### EPISODE 336###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.01973885  0.07673842]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.968725
actor_epsilon 4: 0.821428571429


### EPISODE 337###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00659915  0.06325957]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00976107  0.06009732]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.01292279  0.05693508]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.01608329  0.05377283]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.96865
actor_epsilon 6: 0.875
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.01292169  0.0506106 ]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.00975972  0.04744837]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.00343549  0.04112403]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16946352  0.10214923]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.968575
actor_epsilon 3: 0.621212121212
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.00288886  0.04112478]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.00605105  0.04428696]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.00921324  0.04744917]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01237544  0.0506114 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9685
actor_epsilon 5: 0.845070422535
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.01553763  0.05377363]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.25332034  0.15934503]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.968425
actor_epsilon 3: 0.611940298507
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.02186198  0.06009811]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.02502413  0.06326035]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.96835
actor_epsilon 5: 0.833333333333
Exploring

New Goal: 1
State-Actions: 
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.968275
actor_epsilon 1: 0.1


### EPISODE 338###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.02818653  0.0790716 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9682
actor_epsilon 6: 0.876712328767


### EPISODE 339###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.02502443  0.08223385]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.968125
actor_epsilon 6: 0.878378378378


### EPISODE 340###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.0724398   0.11534907]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96805
actor_epsilon 2: 0.66265060241


### EPISODE 341###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.10155181  0.13704181]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967975
actor_epsilon 4: 0.824561403509


### EPISODE 342###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0155379   0.09172057]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9679
actor_epsilon 5: 0.835616438356


### EPISODE 343###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967825
actor_epsilon 4: 0.827586206897


### EPISODE 344###

New Goal: 2
State-Actions: 
Here ------> [[ 0.04706353  0.11933727]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96775
actor_epsilon 2: 0.666666666667


### EPISODE 345###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.00605127  0.10120729]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967675
actor_epsilon 5: 0.837837837838


### EPISODE 346###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00288906  0.10436951]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00027315  0.10753173]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00343535  0.11069395]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9676
actor_epsilon 6: 0.88


### EPISODE 347###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.0268136   0.13118856]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967525
actor_epsilon 4: 0.830508474576


### EPISODE 348###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.96745
actor_epsilon 1: 0.1


### EPISODE 349###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967375
actor_epsilon 2: 0.670588235294


### EPISODE 350###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9673
actor_epsilon 1: 0.1


### EPISODE 351###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.01924591  0.12650499]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967225
actor_epsilon 6: 0.881578947368


### EPISODE 352###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01608554  0.12966718]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01292362  0.13282938]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.00976156  0.13599159]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.96715
actor_epsilon 5: 0.826666666667
Exploring

New Goal: 1
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.967075
actor_epsilon 1: 0.1


### EPISODE 353###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.967
actor_epsilon 3: 0.617647058824


### EPISODE 354###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.966925
actor_epsilon 2: 0.674418604651


### EPISODE 355###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.01237371  0.15812685]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96685
actor_epsilon 5: 0.828947368421


### EPISODE 356###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.966775
actor_epsilon 2: 0.67816091954


### EPISODE 357###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.13734826  0.19293998]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9667
actor_epsilon 4: 0.833333333333


### EPISODE 358###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.02186023  0.16761284]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.966625
actor_epsilon 5: 0.831168831169


### EPISODE 359###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.16879438  0.20261782]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.63735998  0.33034006]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.18620487  0.20290905]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.65244359  0.34252948]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.17375511  0.19960728]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.59879875  0.34657305]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14059462  0.19002974]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.53653014  0.34478787]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.47658548  0.34040982]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.07810569  0.166198  ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96655
actor_epsilon 4: 0.83606557377


### EPISODE 360###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.966475
actor_epsilon 1: 0.1


### EPISODE 361###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00027819  0.1265063 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00660266  0.1201818 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0097649   0.11701955]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.01292714  0.1138573 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9664
actor_epsilon 5: 0.820512820513
Exploring

New Goal: 4
State-Actions: 
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.966325
actor_epsilon 4: 0.822580645161
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.15554616  0.20197204]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.01625906  0.12488611]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.02557602  0.1012083 ]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.00581629  0.11565533]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.10168487  0.16146985]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.96625
actor_epsilon 3: 0.608695652174
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.16774757  0.18605533]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.966175
actor_epsilon 4: 0.809523809524
Exploring

New Goal: 1
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.9661
actor_epsilon 1: 0.1


### EPISODE 362###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.86162853  0.50491798]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.966025
actor_epsilon 1: 0.1


### EPISODE 363###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01925108  0.06958625]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96595
actor_epsilon 5: 0.822784810127


### EPISODE 364###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.01608886  0.06642495]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.965875
actor_epsilon 6: 0.883116883117


### EPISODE 365###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 1.11004364  0.65524226]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9658
actor_epsilon 1: 0.1


### EPISODE 366###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0028845   0.08539668]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.00604672  0.08855884]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.00920893  0.09172102]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.965725
actor_epsilon 6: 0.884615384615


### EPISODE 367###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.96565
actor_epsilon 2: 0.670454545455
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.01869543  0.10120758]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.965575
actor_epsilon 5: 0.825


### EPISODE 368###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.16773912  0.1558266 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9655
actor_epsilon 4: 0.8125


### EPISODE 369###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.16785592  0.15863466]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.965425
actor_epsilon 4: 0.815384615385


### EPISODE 370###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.01907299  0.11186854]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.96535
actor_epsilon 2: 0.662921348315
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.965275
actor_epsilon 3: 0.614285714286


### EPISODE 371###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9652
actor_epsilon 1: 0.1


### EPISODE 372###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.30877161  0.21109203]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.965125
actor_epsilon 3: 0.619718309859


### EPISODE 373###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00027343  0.12650508]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96505
actor_epsilon 5: 0.827160493827


### EPISODE 374###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.964975
actor_epsilon 1: 0.1


### EPISODE 375###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.02557106  0.15180278]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9649
actor_epsilon 2: 0.666666666667


### EPISODE 376###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.02873321  0.15496498]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.964825
actor_epsilon 5: 0.829268292683


### EPISODE 377###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.03189531  0.15812719]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96475
actor_epsilon 6: 0.886075949367


### EPISODE 378###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.03505721  0.16128938]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.02873415  0.16761373]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.025572   0.1707758]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.02240982  0.17393756]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01924761  0.17077579]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.0160854  0.1676137]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.964675
actor_epsilon 5: 0.819277108434
Exploring

New Goal: 1
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9646
actor_epsilon 1: 0.1


### EPISODE 379###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00027424  0.15180263]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.002888    0.14864039]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.00605024  0.14547816]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.964525
actor_epsilon 5: 0.821428571429


### EPISODE 380###

New Goal: 2
State-Actions: 
Here ------> [[ 0.00921248  0.14231591]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96445
actor_epsilon 2: 0.67032967033


### EPISODE 381###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.29681903  0.28594801]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.964375
actor_epsilon 3: 0.625


### EPISODE 382###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9643
actor_epsilon 1: 0.1


### EPISODE 383###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.30190113  0.26466328]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.964225
actor_epsilon 3: 0.630136986301


### EPISODE 384###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.30407929  0.25425935]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96415
actor_epsilon 3: 0.635135135135


### EPISODE 385###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.964075
actor_epsilon 3: 0.64


### EPISODE 386###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.02818594  0.1233424 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.964
actor_epsilon 2: 0.673913043478


### EPISODE 387###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.03134818  0.12018014]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03451042  0.11701789]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03767265  0.11385563]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04083489  0.11069337]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.963925
actor_epsilon 6: 0.875
Exploring

New Goal: 2
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04715935  0.10436886]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.05032158  0.10120661]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.0534838   0.09804436]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05664602  0.09488211]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.05169404  0.32167313]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.96385
actor_epsilon 2: 0.666666666667
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.963775
actor_epsilon 3: 0.644736842105


### EPISODE 388###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08008435  0.0889158 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08802319  0.08714318]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.07245701  0.07907099]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9637
actor_epsilon 4: 0.818181818182


### EPISODE 389###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.963625
actor_epsilon 3: 0.649350649351


### EPISODE 390###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.96355
actor_epsilon 1: 0.1


### EPISODE 391###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.08194326  0.07590909]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.963475
actor_epsilon 5: 0.823529411765


### EPISODE 392###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0851048   0.07907126]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9634
actor_epsilon 5: 0.825581395349


### EPISODE 393###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08194353  0.08223347]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.963325
actor_epsilon 4: 0.805970149254
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.07561953  0.08855791]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.96325
actor_epsilon 5: 0.816091954023
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0724574   0.09172015]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.963175
actor_epsilon 6: 0.864197530864
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.06929525  0.09488239]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.06613307  0.09804464]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.06297088  0.10120689]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05980868  0.10436914]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05348426  0.11069366]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05032204  0.11385591]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04715982  0.11701817]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.0439976   0.12018043]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04083538  0.12334269]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03767316  0.12650494]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03451093  0.12966721]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03134871  0.13282947]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.02502425  0.139154  ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.02186203  0.14231627]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01869981  0.14547853]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01553759  0.1486408 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.01237537  0.15180306]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.00605095  0.15812759]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.9631
actor_epsilon 6: 0.865853658537


### EPISODE 394###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.00288876  0.16128984]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00027341  0.16445209]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00343556  0.16761434]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.963025
actor_epsilon 6: 0.867469879518


### EPISODE 395###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00659765  0.17077659]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00975955  0.17393884]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.01291847  0.17710109]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00659446  0.18342558]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96295
actor_epsilon 6: 0.869047619048


### EPISODE 396###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00343232  0.18658781]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00027016  0.18975003]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.00289202  0.19291222]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.0092164   0.19923578]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0123786  0.1960737]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01554079  0.19291151]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.01870299  0.18974929]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.962875
actor_epsilon 6: 0.858823529412
Exploring

New Goal: 4
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.72380745  0.4986124 ]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9628
actor_epsilon 4: 0.794117647059
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.02818958  0.18026258]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.962725
actor_epsilon 1: 0.1


### EPISODE 397###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.04400059  0.16445133]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04716279  0.16128908]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.050325    0.15812683]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96265
actor_epsilon 6: 0.860465116279


### EPISODE 398###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.0534872   0.15496458]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.962575
actor_epsilon 3: 0.653846153846


### EPISODE 399###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.05664941  0.15180233]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9625
actor_epsilon 3: 0.645569620253
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05981161  0.14864008]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06297382  0.14547783]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.962425
actor_epsilon 5: 0.818181818182

[[ 0.4    0.561  0.258  0.149  0.086  0.034]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 400###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.06613602  0.14231558]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96235
actor_epsilon 3: 0.65


### EPISODE 401###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.962275
actor_epsilon 4: 0.797101449275


### EPISODE 402###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9622
actor_epsilon 2: 0.670212765957


### EPISODE 403###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.30579627  0.19847067]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.962125
actor_epsilon 4: 0.8


### EPISODE 404###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.07878474  0.12966658]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.96205
actor_epsilon 2: 0.663157894737
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.24311069  0.16414076]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.961975
actor_epsilon 4: 0.802816901408


### EPISODE 405###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.08827041  0.12017983]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08510978  0.11701758]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.08194795  0.11385533]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07878593  0.11069309]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07562386  0.10753085]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9619
actor_epsilon 5: 0.808988764045
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.67522514  0.22946705]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.961825
actor_epsilon 4: 0.791666666667
Exploring

New Goal: 5
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0661375   0.09804419]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06297533  0.09488202]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05981315  0.09171993]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05665095  0.08855809]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.96175
actor_epsilon 5: 0.8
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05348874  0.09171997]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05032653  0.09488211]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.961675
actor_epsilon 5: 0.791208791209
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.04716431  0.09804429]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.04400209  0.1012065 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04083987  0.10436872]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03767765  0.10753095]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.03451543  0.11069319]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03135321  0.11385544]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.028191    0.11701769]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.9616
actor_epsilon 6: 0.850574712644
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.02502878  0.12017994]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.73815995  0.32556915]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.961525
actor_epsilon 3: 0.641975308642
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.01554212  0.12966669]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.74799162  0.34269923]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.96145
actor_epsilon 3: 0.634146341463
Exploring

New Goal: 1
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.961375
actor_epsilon 1: 0.1


### EPISODE 406###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.00289325  0.14231569]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00026897  0.14547794]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00343118  0.14864019]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9613
actor_epsilon 4: 0.794520547945


### EPISODE 407###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.00659339  0.15180244]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.961225
actor_epsilon 2: 0.65625
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.01291778  0.15812694]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96115
actor_epsilon 6: 0.852272727273


### EPISODE 408###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 1.06715775  0.52632797]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.961075
actor_epsilon 1: 0.1


### EPISODE 409###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.02556639  0.17077594]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.02872849  0.17393818]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.0350526   0.18026268]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.04137515  0.18658718]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.03821338  0.18974943]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.961
actor_epsilon 5: 0.782608695652
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.03505133  0.19291168]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.03188921  0.19607392]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.960925
actor_epsilon 5: 0.774193548387
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.82338786  0.51002312]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.96085
actor_epsilon 4: 0.783783783784
Exploring

New Goal: 1
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.960775
actor_epsilon 1: 0.1


### EPISODE 410###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.01607824  0.2118848 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01291603  0.20872302]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.00975382  0.20556088]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9607
actor_epsilon 6: 0.85393258427


### EPISODE 411###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.960625
actor_epsilon 4: 0.786666666667


### EPISODE 412###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00342938  0.19923647]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.96055
actor_epsilon 6: 0.855555555556


### EPISODE 413###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.00026716  0.19607423]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.960475
actor_epsilon 2: 0.659793814433


### EPISODE 414###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9604
actor_epsilon 3: 0.626506024096
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.00605731  0.18974976]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.00921954  0.18658751]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01238179  0.18342526]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.960325
actor_epsilon 4: 0.776315789474
Exploring

New Goal: 2
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.77940077  0.49008268]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.96025
actor_epsilon 2: 0.65306122449
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.960175
actor_epsilon 1: 0.1


### EPISODE 415###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.02503074  0.17077626]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9601
actor_epsilon 3: 0.630952380952


### EPISODE 416###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.02819298  0.16761401]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03135521  0.16445176]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.960025
actor_epsilon 4: 0.779220779221


### EPISODE 417###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.03767967  0.15812725]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95995
actor_epsilon 2: 0.646464646465
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.959875
actor_epsilon 5: 0.776595744681


### EPISODE 418###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9598
actor_epsilon 4: 0.782051282051


### EPISODE 419###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.05032861  0.14547819]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05349085  0.14231592]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05665309  0.13915366]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05981533  0.13599139]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.959725
actor_epsilon 4: 0.772151898734
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.06297757  0.13282913]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95965
actor_epsilon 2: 0.64
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.959575
actor_epsilon 4: 0.775


### EPISODE 420###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07246429  0.12334234]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9595
actor_epsilon 3: 0.623529411765
Exploring

New Goal: 2
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.959425
actor_epsilon 2: 0.633663366337
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07878873  0.11701783]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95935
actor_epsilon 6: 0.857142857143


### EPISODE 421###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.959275
actor_epsilon 3: 0.627906976744


### EPISODE 422###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9592
actor_epsilon 3: 0.632183908046


### EPISODE 423###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.08827539  0.10753106]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.959125
actor_epsilon 3: 0.636363636364


### EPISODE 424###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95905
actor_epsilon 2: 0.627450980392
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.09776203  0.09804432]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.958975
actor_epsilon 2: 0.631067961165


### EPISODE 425###

New Goal: 2
State-Actions: 
Here ------> [[ 0.10092425  0.09488209]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9589
actor_epsilon 2: 0.634615384615


### EPISODE 426###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.958825
actor_epsilon 1: 0.1


### EPISODE 427###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.11357283  0.08223319]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95875
actor_epsilon 5: 0.778947368421


### EPISODE 428###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.958675
actor_epsilon 3: 0.640449438202


### EPISODE 429###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9586
actor_epsilon 4: 0.777777777778


### EPISODE 430###

New Goal: 2
State-Actions: 
Here ------> [[ 0.11041044  0.07274675]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.958525
actor_epsilon 2: 0.638095238095


### EPISODE 431###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.10724843  0.06958713]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95845
actor_epsilon 3: 0.644444444444


### EPISODE 432###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.10408646  0.07274924]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.958375
actor_epsilon 2: 0.641509433962


### EPISODE 433###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9583
actor_epsilon 1: 0.1


### EPISODE 434###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.09776251  0.07907366]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.958225
actor_epsilon 2: 0.644859813084


### EPISODE 435###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0914383   0.08539814]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08827613  0.08856038]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95815
actor_epsilon 5: 0.78125


### EPISODE 436###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.958075
actor_epsilon 1: 0.1


### EPISODE 437###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.958
actor_epsilon 2: 0.648148148148


### EPISODE 438###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.957925
actor_epsilon 1: 0.1


### EPISODE 439###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07562725  0.10120938]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95785
actor_epsilon 6: 0.858695652174


### EPISODE 440###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.07246502  0.10437164]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.06614055  0.11069615]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.06297832  0.11385841]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.05981609  0.11702067]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.05665386  0.12018292]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.05032941  0.12650745]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.04716717  0.12966971]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.03768044  0.13915651]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03451819  0.14231877]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.71517444  0.41152608]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.957775
actor_epsilon 2: 0.642201834862
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.0281937  0.1486433]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02503147  0.15180556]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.02186924  0.15496783]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01870701  0.15813009]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9577
actor_epsilon 4: 0.768292682927
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 1.45076191  0.6823386 ]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.957625
actor_epsilon 3: 0.637362637363
Exploring

New Goal: 4
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.95755
actor_epsilon 4: 0.759036144578
Exploring

New Goal: 1
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.957475
actor_epsilon 1: 0.1


### EPISODE 441###

New Goal: 2
State-Actions: 
Here ------> [[-0.00026641  0.17710368]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.55614513  0.36665091]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9574
actor_epsilon 2: 0.636363636364
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.957325
actor_epsilon 1: 0.1


### EPISODE 442###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[-0.00975311  0.18659043]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.45762202  0.33966237]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95725
actor_epsilon 2: 0.630630630631
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.01607751  0.19291493]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.01923969  0.19607718]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.02240187  0.19923943]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.957175
actor_epsilon 5: 0.773195876289
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.02556406  0.20240168]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.9571
actor_epsilon 6: 0.849462365591
Exploring

New Goal: 5
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.957025
actor_epsilon 5: 0.765306122449
Exploring

New Goal: 4
State-Actions: 
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.95695
actor_epsilon 4: 0.75
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.03505066  0.21188842]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.03821284  0.21505065]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.04137497  0.21821289]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.956875
actor_epsilon 6: 0.851063829787


### EPISODE 443###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9568
actor_epsilon 4: 0.752941176471


### EPISODE 444###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.04137616  0.22453733]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.956725
actor_epsilon 6: 0.852631578947


### EPISODE 445###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95665
actor_epsilon 2: 0.633928571429


### EPISODE 446###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.03505206  0.23086171]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.03188994  0.23402382]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[-0.02872784  0.23718578]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[-0.02556576  0.23402388]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.956575
actor_epsilon 6: 0.84375
Exploring

New Goal: 5
State-Actions: 
(6, 0); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9565
actor_epsilon 5: 0.757575757576
Exploring

New Goal: 5
State-Actions: 
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[-0.01607956  0.22453734]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.956425
actor_epsilon 5: 0.75
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.82868057  0.4255493 ]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.95635
actor_epsilon 4: 0.744186046512
Exploring

New Goal: 1
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 1.27620518  0.48200715]]
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.956275
actor_epsilon 1: 0.1


### EPISODE 447###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9562
actor_epsilon 6: 0.845360824742


### EPISODE 448###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.956125
actor_epsilon 1: 0.1


### EPISODE 449###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95605
actor_epsilon 6: 0.84693877551


### EPISODE 450###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.955975
actor_epsilon 6: 0.848484848485


### EPISODE 451###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02819126  0.18026592]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03135349  0.17710367]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03451572  0.17394142]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.03767794  0.17077917]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9559
actor_epsilon 6: 0.85


### EPISODE 452###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.04084016  0.16761692]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.56390154  0.48890665]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.955825
actor_epsilon 2: 0.628318584071
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.04716459  0.16129242]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05032679  0.15813017]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.95575
actor_epsilon 4: 0.735632183908
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 1.38132024  0.45708394]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.955675
actor_epsilon 3: 0.630434782609
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05665123  0.15180567]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05981344  0.14864342]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9556
actor_epsilon 5: 0.752475247525


### EPISODE 453###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06297566  0.14548117]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.06613787  0.14231893]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06930006  0.1391567 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07246221  0.13599446]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.07562426  0.13283224]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.955525
actor_epsilon 5: 0.754901960784


### EPISODE 454###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0787861   0.12967004]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95545
actor_epsilon 6: 0.851485148515


### EPISODE 455###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.955375
actor_epsilon 1: 0.1


### EPISODE 456###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08510989  0.12334575]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9553
actor_epsilon 4: 0.738636363636


### EPISODE 457###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.08827199  0.12650639]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.46277153  0.23630485]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.955225
actor_epsilon 2: 0.622807017544
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.95515
actor_epsilon 1: 0.1


### EPISODE 458###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.09775846  0.13599275]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.955075
actor_epsilon 3: 0.634408602151


### EPISODE 459###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.10408238  0.14231718]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.10092048  0.1454794 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.09459618  0.15180384]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.955
actor_epsilon 5: 0.757281553398


### EPISODE 460###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.09143399  0.15496607]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08827179  0.15812831]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08510959  0.16129054]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08194739  0.16445278]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07878519  0.16761501]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07246076  0.17393948]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.06929854  0.17710172]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06613632  0.18026395]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.954925
actor_epsilon 6: 0.852941176471


### EPISODE 461###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05664963  0.18975066]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05348741  0.19291289]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05032519  0.19607511]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04716298  0.19923733]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.04400077  0.20239955]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04083857  0.20556176]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03767637  0.20872393]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03451416  0.21188608]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.03135194  0.21504757]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02818971  0.2118856 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02186526  0.20556122]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95485
actor_epsilon 5: 0.759615384615


### EPISODE 462###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.954775
actor_epsilon 1: 0.1


### EPISODE 463###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9547
actor_epsilon 1: 0.1


### EPISODE 464###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.954625
actor_epsilon 1: 0.1


### EPISODE 465###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00659402  0.17710106]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95455
actor_epsilon 5: 0.761904761905


### EPISODE 466###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00975979  0.16761431]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.954475
actor_epsilon 6: 0.854368932039


### EPISODE 467###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.0065978   0.16445206]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9544
actor_epsilon 6: 0.855769230769


### EPISODE 468###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00343566  0.16128981]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00027347  0.15812756]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.00288873  0.15496531]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.00605094  0.15180306]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.954325
actor_epsilon 6: 0.847619047619
Exploring

New Goal: 3
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.01237536  0.14547856]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.01553756  0.14231631]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.02186191  0.13599181]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 1.48665321  0.7714994 ]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.95425
actor_epsilon 3: 0.627659574468
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.954175
actor_epsilon 1: 0.1


### EPISODE 469###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04083509  0.1170183 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.04399732  0.11385606]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9541
actor_epsilon 5: 0.754716981132
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 1.07672107  0.54527682]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.954025
actor_epsilon 4: 0.730337078652
Exploring

New Goal: 1
State-Actions: 
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.06929493  0.08855899]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09775444  0.11701896]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09775128  0.12334348]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.09458922  0.12650573]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.95395
actor_epsilon 1: 0.1


### EPISODE 470###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.02493656  0.46178481]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.953875
actor_epsilon 1: 0.1


### EPISODE 471###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9538
actor_epsilon 3: 0.621052631579
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06613006  0.15496612]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.06296787  0.15812838]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.953725
actor_epsilon 5: 0.747663551402
Exploring

New Goal: 2
State-Actions: 
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05031903  0.17077744]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04715681  0.1739397 ]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04083239  0.18026423]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.0376702  0.1834265]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.62494868  0.34878159]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95365
actor_epsilon 2: 0.617391304348
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.953575
actor_epsilon 1: 0.1


### EPISODE 472###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9535
actor_epsilon 6: 0.849056603774


### EPISODE 473###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.02502181  0.19607556]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.94102699  0.44602019]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.953425
actor_epsilon 2: 0.612068965517
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.1204915   0.52183115]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95335
actor_epsilon 2: 0.606837606838
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.26649049  0.28376195]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.953275
actor_epsilon 4: 0.733333333333


### EPISODE 474###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.25610057  0.28705814]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0060492   0.21504915]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.23332746  0.29269949]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9532
actor_epsilon 4: 0.736263736264


### EPISODE 475###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[-0.00027517  0.22137368]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.953125
actor_epsilon 3: 0.625


### EPISODE 476###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.20832524  0.29089445]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95305
actor_epsilon 4: 0.739130434783


### EPISODE 477###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.00659949  0.22769821]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00976159  0.23086047]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[-0.01292342  0.23402272]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.952975
actor_epsilon 6: 0.85046728972


### EPISODE 478###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9529
actor_epsilon 3: 0.628865979381


### EPISODE 479###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.04789495  0.25683245]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00343738  0.24350947]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.952825
actor_epsilon 4: 0.731182795699
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.0002752   0.24667172]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.00288699  0.24983396]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95275
actor_epsilon 5: 0.75


### EPISODE 480###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08144119  0.28171137]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01237359  0.25932053]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05961598  0.27762043]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01869798  0.25931978]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.03854389  0.26170918]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.952675
actor_epsilon 4: 0.734042553191


### EPISODE 481###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.02502234  0.25299537]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03134663  0.2466709 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.99132788  0.54043055]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9526
actor_epsilon 2: 0.601694915254
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.952525
actor_epsilon 3: 0.632653061224


### EPISODE 482###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.040833    0.23718415]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95245
actor_epsilon 6: 0.851851851852


### EPISODE 483###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.04399515  0.2340219 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04715732  0.23085964]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05031952  0.22769737]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.05348172  0.22453511]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.952375
actor_epsilon 6: 0.844036697248
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.05664394  0.22137284]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.20998856  0.25734869]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9523
actor_epsilon 4: 0.726315789474
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0629684   0.21504831]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.952225
actor_epsilon 5: 0.743119266055
Exploring

New Goal: 3
State-Actions: 
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.46928227  0.3047455 ]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.95215
actor_epsilon 3: 0.626262626263
Exploring

New Goal: 5
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.07561736  0.20239925]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.952075
actor_epsilon 5: 0.745454545455


### EPISODE 484###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.07877959  0.19923699]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.952
actor_epsilon 2: 0.605042016807


### EPISODE 485###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08194183  0.19607472]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.951925
actor_epsilon 6: 0.845454545455


### EPISODE 486###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.16066611  0.44130638]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.95185
actor_epsilon 2: 0.6
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.09142851  0.18658793]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.951775
actor_epsilon 4: 0.729166666667


### EPISODE 487###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.12730676  0.19190794]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9517
actor_epsilon 4: 0.731958762887


### EPISODE 488###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.951625
actor_epsilon 2: 0.603305785124


### EPISODE 489###

New Goal: 2
State-Actions: 
Here ------> [[ 0.1009151   0.17710114]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95155
actor_epsilon 2: 0.606557377049


### EPISODE 490###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.10723927  0.17077661]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12739591  0.17314728]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.1135591   0.16445208]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.11039746  0.16128981]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.10723549  0.15812755]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.951475
actor_epsilon 6: 0.837837837838
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.10407345  0.15496528]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.10091138  0.15180302]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 1.11078906  0.56900787]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9514
actor_epsilon 3: 0.62
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.09458722  0.14547849]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.951325
actor_epsilon 4: 0.724489795918
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.23034322  0.20593476]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.30380422  0.23953074]]
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.95125
actor_epsilon 4: 0.727272727273


### EPISODE 491###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.951175
actor_epsilon 1: 0.1


### EPISODE 492###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9511
actor_epsilon 4: 0.73


### EPISODE 493###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.09457777  0.1201805 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.951025
actor_epsilon 5: 0.747747747748


### EPISODE 494###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.09141634  0.11701827]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95095
actor_epsilon 2: 0.609756097561


### EPISODE 495###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.08825443  0.11385603]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08193026  0.10753158]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.950875
actor_epsilon 5: 0.75


### EPISODE 496###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07876813  0.10436937]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9508
actor_epsilon 3: 0.623762376238


### EPISODE 497###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.07560597  0.10120717]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.950725
actor_epsilon 2: 0.604838709677
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0692816   0.09488282]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.95065
actor_epsilon 5: 0.752212389381


### EPISODE 498###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.06611941  0.09172072]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.950575
actor_epsilon 6: 0.839285714286


### EPISODE 499###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9505
actor_epsilon 1: 0.1

[[ 0.5    0.719  0.351  0.208  0.128  0.052]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 500###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.950425
actor_epsilon 3: 0.627450980392


### EPISODE 501###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.95035
actor_epsilon 1: 0.1


### EPISODE 502###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.05347066  0.09172397]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.950275
actor_epsilon 2: 0.608


### EPISODE 503###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.05030851  0.09488617]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9502
actor_epsilon 6: 0.840707964602


### EPISODE 504###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.950125
actor_epsilon 1: 0.1


### EPISODE 505###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.04398452  0.10121062]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0471361   0.10437287]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.95005
actor_epsilon 4: 0.722772277228
Exploring

New Goal: 1
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.9312889   0.62825441]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.949975
actor_epsilon 1: 0.1


### EPISODE 506###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9499
actor_epsilon 1: 0.1


### EPISODE 507###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.949825
actor_epsilon 1: 0.1


### EPISODE 508###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.07243305  0.12967092]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.07875749  0.13599545]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08191971  0.13915771]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.94975
actor_epsilon 4: 0.71568627451
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.08508194  0.14231998]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.949675
actor_epsilon 1: 0.1


### EPISODE 509###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.09456863  0.15180677]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9496
actor_epsilon 6: 0.842105263158


### EPISODE 510###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.14297116  0.17704375]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.949525
actor_epsilon 4: 0.718446601942


### EPISODE 511###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.10089308  0.1581313 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.71991664  0.44777474]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.94945
actor_epsilon 2: 0.603174603175

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.949375
actor_epsilon 2: 0.606299212598


### EPISODE 512###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11354198  0.17078036]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9493
actor_epsilon 4: 0.711538461538
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.1167042   0.17394263]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11986642  0.17710489]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12302864  0.18026716]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.949225
actor_epsilon 5: 0.745614035088
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.24553484  0.22717841]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12935305  0.18659168]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.94915
actor_epsilon 5: 0.739130434783
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.61713737  0.35033923]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.949075
actor_epsilon 4: 0.704761904762
Exploring

New Goal: 4
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.17763162  0.20740567]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.949
actor_epsilon 4: 0.698113207547
Exploring

New Goal: 3
State-Actions: 
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.65597022  0.31898358]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.948925
actor_epsilon 3: 0.621359223301
Exploring

New Goal: 1
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.94885
actor_epsilon 1: 0.1


### EPISODE 513###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.948775
actor_epsilon 2: 0.609375


### EPISODE 514###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13567165  0.23718792]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.13250959  0.24035019]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12849393  0.24743557]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.12302597  0.24983698]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1344734   0.25665289]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14895268  0.26584765]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.13567303  0.26248604]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.16560911  0.27286801]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.14199659  0.26881057]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9487
actor_epsilon 5: 0.73275862069
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.14515631  0.27197284]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.14199457  0.2751351 ]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.13883255  0.27829736]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12934603  0.28778413]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.12618381  0.29094636]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12302157  0.2941086 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.69775212  0.34367245]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.948625
actor_epsilon 2: 0.604651162791
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.1166971   0.30043307]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.94855
actor_epsilon 3: 0.625


### EPISODE 515###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.15925975  0.3117629 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.60953218  0.42397183]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.948475
actor_epsilon 2: 0.6
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.14838339  0.32212979]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9484
actor_epsilon 2: 0.603053435115


### EPISODE 516###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.27939415  0.35226962]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.19809705  0.32482052]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.948325
actor_epsilon 4: 0.691588785047
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.44857585  0.36089095]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.186764   0.3071419]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.09139919  0.29410481]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94825
actor_epsilon 6: 0.84347826087


### EPISODE 517###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.948175
actor_epsilon 4: 0.694444444444


### EPISODE 518###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.12670535  0.28961626]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9481
actor_epsilon 2: 0.606060606061


### EPISODE 519###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.948025
actor_epsilon 4: 0.697247706422


### EPISODE 520###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.09621941  0.27982363]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.10274491  0.27960679]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.09084991  0.27516061]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94795
actor_epsilon 5: 0.735042735043


### EPISODE 521###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.08809531  0.27277142]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.947875
actor_epsilon 5: 0.737288135593


### EPISODE 522###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9478
actor_epsilon 3: 0.628571428571


### EPISODE 523###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.27003786  0.32999128]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.947725
actor_epsilon 4: 0.7


### EPISODE 524###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.10831337  0.27010551]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.94765
actor_epsilon 2: 0.601503759398
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06743449  0.25030455]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.947575
actor_epsilon 5: 0.739495798319


### EPISODE 525###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.26126164  0.28960985]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9475
actor_epsilon 4: 0.702702702703


### EPISODE 526###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.947425
actor_epsilon 4: 0.705357142857


### EPISODE 527###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0577968   0.23815332]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94735
actor_epsilon 5: 0.741666666667


### EPISODE 528###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.25234771  0.25918961]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.947275
actor_epsilon 4: 0.70796460177


### EPISODE 529###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9472
actor_epsilon 4: 0.710526315789


### EPISODE 530###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.02183077  0.22453502]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.947125
actor_epsilon 3: 0.632075471698


### EPISODE 531###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94705
actor_epsilon 3: 0.635514018692


### EPISODE 532###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.946975
actor_epsilon 2: 0.60447761194


### EPISODE 533###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9469
actor_epsilon 1: 0.1


### EPISODE 534###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.946825
actor_epsilon 1: 0.1


### EPISODE 535###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.02961969  0.21381857]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01137612  0.20763077]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.01814097  0.20740138]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[-0.00046502  0.20013979]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.94675
actor_epsilon 5: 0.735537190083
Exploring

New Goal: 1
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.946675
actor_epsilon 1: 0.1


### EPISODE 536###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[-0.0224397   0.18026331]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9466
actor_epsilon 6: 0.844827586207


### EPISODE 537###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.04111609  0.20577058]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.94142306  0.64019501]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.946525
actor_epsilon 2: 0.6
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94645
actor_epsilon 3: 0.638888888889


### EPISODE 538###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.946375
actor_epsilon 4: 0.713043478261


### EPISODE 539###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[-0.00662931  0.16445199]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9463
actor_epsilon 5: 0.737704918033


### EPISODE 540###

New Goal: 2
State-Actions: 
Here ------> [[ 0.05955669  0.19473156]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.946225
actor_epsilon 2: 0.595588235294
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.00601962  0.15180293]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94615
actor_epsilon 6: 0.846153846154


### EPISODE 541###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0123441  0.1454784]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01550635  0.14231613]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.946075
actor_epsilon 5: 0.739837398374


### EPISODE 542###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0249931   0.13282934]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.02815535  0.12966707]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.946
actor_epsilon 5: 0.733870967742
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.2776069   0.25363556]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03447986  0.12334255]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.03764211  0.12018029]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.945925
actor_epsilon 5: 0.736


### EPISODE 543###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.94585
actor_epsilon 1: 0.1


### EPISODE 544###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.0534534   0.10436915]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.945775
actor_epsilon 3: 0.633027522936
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08136822  0.10943308]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05977791  0.1043679 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.13166875  0.13172069]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06610242  0.11069213]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9457
actor_epsilon 6: 0.847457627119


### EPISODE 545###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.945625
actor_epsilon 2: 0.598540145985


### EPISODE 546###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.17331101  0.15528834]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94555
actor_epsilon 5: 0.738095238095


### EPISODE 547###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07558913  0.12017874]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.945475
actor_epsilon 3: 0.636363636364


### EPISODE 548###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07875134  0.12334097]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9454
actor_epsilon 6: 0.848739495798


### EPISODE 549###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.945325
actor_epsilon 3: 0.63963963964


### EPISODE 550###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.49407971  0.563815  ]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.94525
actor_epsilon 2: 0.594202898551
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.945175
actor_epsilon 2: 0.597122302158


### EPISODE 551###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9451
actor_epsilon 1: 0.1


### EPISODE 552###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07875862  0.14863898]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.945025
actor_epsilon 6: 0.85


### EPISODE 553###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08192001  0.15180124]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08508191  0.15496351]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.08824398  0.15812577]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09140614  0.16128804]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.09456831  0.1644503 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0977305   0.16761257]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.10089271  0.17077483]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.10405492  0.1739371 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.10721714  0.17709936]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.11354159  0.18342389]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94495
actor_epsilon 6: 0.851239669421


### EPISODE 554###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11986604  0.18974842]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.944875
actor_epsilon 4: 0.706896551724
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.12302827  0.19291069]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1261905   0.19607295]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9448
actor_epsilon 5: 0.740157480315


### EPISODE 555###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.944725
actor_epsilon 1: 0.1


### EPISODE 556###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94465
actor_epsilon 4: 0.709401709402


### EPISODE 557###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.1388393   0.20872201]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14200145  0.21188428]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14516355  0.21504654]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.15148552  0.22137107]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.14832363  0.22453333]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1419995   0.23085786]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.13883738  0.23402013]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.13567528  0.23718239]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.13251317  0.24034466]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12302695  0.24983145]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.12300771  0.25615597]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.944575
actor_epsilon 6: 0.844262295082
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12616967  0.25931823]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.3127856   0.31278348]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9445
actor_epsilon 3: 0.633928571429
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13565612  0.26880503]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.13881832  0.27196729]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14198053  0.27512956]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.14514275  0.27829182]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14830497  0.28145409]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.15462941  0.28777862]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.15779163  0.29094088]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.944425
actor_epsilon 5: 0.7421875


### EPISODE 558###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.171874    0.29431897]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.17044041  0.30358994]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.94435
actor_epsilon 4: 0.703389830508
Exploring

New Goal: 3
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.944275
actor_epsilon 3: 0.628318584071
Exploring

New Goal: 5
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.18266146  0.31325492]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9442
actor_epsilon 5: 0.744186046512


### EPISODE 559###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.944125
actor_epsilon 1: 0.1


### EPISODE 560###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.94405
actor_epsilon 1: 0.1


### EPISODE 561###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.943975
actor_epsilon 4: 0.705882352941


### EPISODE 562###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9439
actor_epsilon 4: 0.708333333333


### EPISODE 563###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.26573804  0.36343753]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.943825
actor_epsilon 4: 0.710743801653


### EPISODE 564###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.94375
actor_epsilon 1: 0.1


### EPISODE 565###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.15146586  0.33521202]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.943675
actor_epsilon 3: 0.631578947368


### EPISODE 566###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9436
actor_epsilon 4: 0.713114754098


### EPISODE 567###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.943525
actor_epsilon 1: 0.1


### EPISODE 568###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.14197974  0.32572535]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.1419808   0.31940085]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.94345
actor_epsilon 6: 0.837398373984
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.14830486  0.31307632]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.15146698  0.30991405]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1654522   0.30801374]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.1609534   0.30042726]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.19546059  0.31602949]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.16727765  0.29410273]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.20521903  0.31076869]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.943375
actor_epsilon 6: 0.838709677419


### EPISODE 569###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.02278745  0.70652807]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9433
actor_epsilon 1: 0.1


### EPISODE 570###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.20400585  0.29266068]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.943225
actor_epsilon 5: 0.746153846154


### EPISODE 571###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.94315
actor_epsilon 1: 0.1


### EPISODE 572###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.19439375  0.28350818]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.943075
actor_epsilon 5: 0.748091603053


### EPISODE 573###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.943
actor_epsilon 4: 0.715447154472


### EPISODE 574###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.16727686  0.26880461]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.71604484  0.44157097]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.942925
actor_epsilon 2: 0.592857142857

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94285
actor_epsilon 2: 0.595744680851


### EPISODE 575###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.942775
actor_epsilon 2: 0.591549295775
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.50482744  0.33109105]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9427
actor_epsilon 4: 0.717741935484


### EPISODE 576###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.942625
actor_epsilon 2: 0.594405594406


### EPISODE 577###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.94255
actor_epsilon 1: 0.1


### EPISODE 578###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12300649  0.2245329 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.942475
actor_epsilon 3: 0.634782608696


### EPISODE 579###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9424
actor_epsilon 1: 0.1


### EPISODE 580###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.942325
actor_epsilon 2: 0.597222222222


### EPISODE 581###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.11352002  0.21504611]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94225
actor_epsilon 6: 0.84


### EPISODE 582###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.11035784  0.21188384]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.942175
actor_epsilon 6: 0.84126984127


### EPISODE 583###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.10719565  0.20872158]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9421
actor_epsilon 6: 0.842519685039


### EPISODE 584###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.10403346  0.20555931]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.942025
actor_epsilon 2: 0.6


### EPISODE 585###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.10087124  0.20239705]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09770902  0.19923478]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.09138456  0.19291025]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.94195
actor_epsilon 6: 0.8359375
Exploring

New Goal: 1
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 1.1857059   0.51779199]]
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.941875
actor_epsilon 1: 0.1


### EPISODE 586###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.07241112  0.17393667]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9418
actor_epsilon 5: 0.75


### EPISODE 587###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.06608664  0.16761214]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.941725
actor_epsilon 1: 0.1


### EPISODE 588###

New Goal: 2
State-Actions: 
Here ------> [[ 0.04711332  0.14863861]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94165
actor_epsilon 2: 0.602739726027


### EPISODE 589###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.04395116  0.14547637]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.941575
actor_epsilon 6: 0.837209302326


### EPISODE 590###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9415
actor_epsilon 3: 0.637931034483


### EPISODE 591###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.941425
actor_epsilon 3: 0.641025641026


### EPISODE 592###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94135
actor_epsilon 5: 0.751879699248


### EPISODE 593###

New Goal: 2
State-Actions: 
Here ------> [[ 0.04395156  0.13915233]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.941275
actor_epsilon 2: 0.605442176871


### EPISODE 594###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.04711372  0.14231451]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05027591  0.14547673]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9412
actor_epsilon 4: 0.72


### EPISODE 595###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.941125
actor_epsilon 3: 0.64406779661


### EPISODE 596###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.05976247  0.15496346]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.94105
actor_epsilon 3: 0.638655462185
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.06292464  0.15812571]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.940975
actor_epsilon 4: 0.714285714286
Exploring

New Goal: 6
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.06924891  0.16445021]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07241102  0.16761248]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.07557315  0.17077474]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.9409
actor_epsilon 6: 0.830769230769
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07873528  0.17393701]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.0818974   0.17709927]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.940825
actor_epsilon 6: 0.824427480916
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08505954  0.18026154]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 1.13580465  0.75025517]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.94075
actor_epsilon 4: 0.708661417323
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 1.08943689  0.74281251]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.940675
actor_epsilon 3: 0.633333333333
Exploring

New Goal: 2
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9406
actor_epsilon 2: 0.601351351351
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.09770825  0.1929106 ]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.940525
actor_epsilon 4: 0.7109375


### EPISODE 597###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94045
actor_epsilon 4: 0.713178294574


### EPISODE 598###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.940375
actor_epsilon 5: 0.753731343284


### EPISODE 599###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9403
actor_epsilon 2: 0.604026845638

[[ 0.6    0.866  0.429  0.253  0.149  0.059]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 600###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.940225
actor_epsilon 1: 0.1


### EPISODE 601###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.11351939  0.20872192]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94015
actor_epsilon 3: 0.636363636364


### EPISODE 602###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.940075
actor_epsilon 5: 0.755555555556


### EPISODE 603###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.94
actor_epsilon 4: 0.715384615385


### EPISODE 604###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.1230061   0.21820872]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.939925
actor_epsilon 3: 0.631147540984
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.93985
actor_epsilon 1: 0.1


### EPISODE 605###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.13249278  0.22769551]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.135655    0.23085777]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.13881722  0.23402004]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14197943  0.2371823 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14830384  0.24350683]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.15146606  0.2466691 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.15462828  0.24983136]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.1577905   0.25299361]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.16095273  0.25615588]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.16411497  0.25931814]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.939775
actor_epsilon 6: 0.825757575758


### EPISODE 606###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.1672772   0.26248041]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9397
actor_epsilon 6: 0.827067669173


### EPISODE 607###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.17043945  0.26564267]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.939625
actor_epsilon 3: 0.626016260163
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.1736017   0.26880494]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.75625432  0.72389346]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.93955
actor_epsilon 3: 0.620967741935
Exploring

New Goal: 6
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.18308845  0.27829173]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1862507  0.281454 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.18941295  0.28461626]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1925752   0.28777853]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.19573745  0.29094079]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.1988997   0.29410303]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.939475
actor_epsilon 6: 0.820895522388
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.20206195  0.29726526]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.2052242  0.3004275]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.21154869  0.30675188]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.21471092  0.30991396]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.21787314  0.30675316]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.22103536  0.3035911 ]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.22735976  0.29726672]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.23052193  0.29410452]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.23368409  0.29094228]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.24000835  0.28461781]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.24633257  0.27829334]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9394
actor_epsilon 2: 0.6
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.939325
actor_epsilon 3: 0.616
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.25581881  0.26880664]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.25898066  0.2656444 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.26214194  0.26248217]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.2589801   0.25931993]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.25581798  0.2561577 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.2526558   0.25299546]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.93925
actor_epsilon 5: 0.75
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.2494936   0.24983321]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.939175
actor_epsilon 1: 0.1


### EPISODE 608###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9391
actor_epsilon 3: 0.619047619048


### EPISODE 609###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22419566  0.22453536]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.22103341  0.22137319]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.21787116  0.21821107]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.939025
actor_epsilon 6: 0.814814814815
Exploring

New Goal: 4
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.93895
actor_epsilon 4: 0.709923664122
Exploring

New Goal: 2
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.58755773  0.30237287]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.938875
actor_epsilon 2: 0.596026490066
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.62345016  0.30985856]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9388
actor_epsilon 2: 0.592105263158
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.19573541  0.23402224]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.18941091  0.24034671]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18624866  0.24350896]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.938725
actor_epsilon 6: 0.816176470588


### EPISODE 610###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.17992413  0.24983346]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93865
actor_epsilon 4: 0.712121212121


### EPISODE 611###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.17676187  0.25299573]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.1735996   0.25615799]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.17043734  0.25932026]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.16727507  0.26248252]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16411281  0.26564479]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.16095054  0.26880705]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.15778828  0.27196932]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.15462601  0.27513158]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.15146375  0.27829385]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14830148  0.28145611]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14513922  0.28461838]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.938575
actor_epsilon 6: 0.817518248175


### EPISODE 612###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9385
actor_epsilon 1: 0.1


### EPISODE 613###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.938425
actor_epsilon 1: 0.1


### EPISODE 614###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93835
actor_epsilon 3: 0.622047244094


### EPISODE 615###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.938275
actor_epsilon 5: 0.751824817518


### EPISODE 616###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.12932789  0.3004297 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9382
actor_epsilon 6: 0.81884057971


### EPISODE 617###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12616563  0.30359197]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.938125
actor_epsilon 3: 0.625


### EPISODE 618###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.12300337  0.3067542 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93805
actor_epsilon 6: 0.820143884892


### EPISODE 619###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.11984111  0.30991644]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.937975
actor_epsilon 4: 0.706766917293
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.1135166   0.31624091]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.10719208  0.32256535]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.10086757  0.32888949]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9379
actor_epsilon 5: 0.753623188406


### EPISODE 620###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.937825
actor_epsilon 4: 0.708955223881


### EPISODE 621###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93775
actor_epsilon 4: 0.711111111111


### EPISODE 622###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.0913808   0.31940299]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08821854  0.31624076]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08505628  0.31307852]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.937675
actor_epsilon 4: 0.713235294118


### EPISODE 623###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08189403  0.30991626]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9376
actor_epsilon 4: 0.715328467153


### EPISODE 624###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.07873177  0.30675399]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.937525
actor_epsilon 3: 0.627906976744


### EPISODE 625###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.07556951  0.30359173]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93745
actor_epsilon 4: 0.717391304348


### EPISODE 626###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.07240725  0.30042946]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.069245   0.2972672]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06608275  0.29410493]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.937375
actor_epsilon 4: 0.719424460432


### EPISODE 627###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.0629205   0.29094267]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9373
actor_epsilon 5: 0.755395683453


### EPISODE 628###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05659601  0.28461814]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.937225
actor_epsilon 1: 0.1


### EPISODE 629###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.05027154  0.27829361]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04710932  0.27513134]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.93715
actor_epsilon 4: 0.714285714286
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.04394713  0.27196908]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.040785    0.26880682]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.937075
actor_epsilon 4: 0.709219858156

New Goal: 2
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.937
actor_epsilon 2: 0.588235294118
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.936925
actor_epsilon 6: 0.821428571429


### EPISODE 630###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93685
actor_epsilon 3: 0.630769230769


### EPISODE 631###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05027185  0.25299549]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.936775
actor_epsilon 5: 0.757142857143


### EPISODE 632###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.05343406  0.24983323]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9367
actor_epsilon 3: 0.63358778626


### EPISODE 633###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.05659628  0.24667096]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06292073  0.24034643]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06924517  0.2340219 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07240738  0.23085964]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07556958  0.22769737]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08189398  0.22137284]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.936625
actor_epsilon 6: 0.822695035461


### EPISODE 634###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.09138045  0.21188608]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09454256  0.20872383]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.09770466  0.20556159]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.93655
actor_epsilon 5: 0.751773049645

New Goal: 2
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.10402878  0.19923714]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.10719081  0.19607499]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.31045079  0.39331606]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.936475
actor_epsilon 2: 0.584415584416

New Goal: 2
State-Actions: 
Here ------> [[ 0.11667719  0.19923837]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12300155  0.20556283]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.17239881  0.53359175]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9364
actor_epsilon 2: 0.58064516129
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.936325
actor_epsilon 5: 0.753521126761


### EPISODE 635###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.13248816  0.21504956]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93625
actor_epsilon 2: 0.583333333333


### EPISODE 636###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.936175
actor_epsilon 3: 0.636363636364


### EPISODE 637###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.13881262  0.22137406]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14197485  0.22453631]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9361
actor_epsilon 4: 0.704225352113
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.14513709  0.22769858]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.14829932  0.23086084]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.15146157  0.23402311]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.936025
actor_epsilon 6: 0.823943661972


### EPISODE 638###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.93595
actor_epsilon 1: 0.1


### EPISODE 639###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.15778607  0.24034764]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.935875
actor_epsilon 5: 0.755244755245


### EPISODE 640###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.9496603   1.10912085]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9358
actor_epsilon 2: 0.579617834395
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.17359732  0.25615895]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.935725
actor_epsilon 3: 0.631578947368

New Goal: 2
State-Actions: 
Here ------> [[ 1.20582652  1.28649282]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.17992182  0.26248348]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.18624632  0.26880801]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.93565
actor_epsilon 2: 0.575949367089
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.20205757  0.28461921]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.20521982  0.28778145]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.20838207  0.29094365]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.21154431  0.29410586]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.21786878  0.30043024]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.935575
actor_epsilon 6: 0.825174825175


### EPISODE 641###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.22103101  0.30359241]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9355
actor_epsilon 4: 0.699300699301
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.69724405  0.49643838]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.935425
actor_epsilon 3: 0.626865671642
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.93535
actor_epsilon 1: 0.1


### EPISODE 642###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.2431667  0.3067551]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.935275
actor_epsilon 6: 0.826388888889


### EPISODE 643###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.2494912   0.30043113]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9352
actor_epsilon 4: 0.694444444444
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.94100499  0.29535684]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.935125
actor_epsilon 3: 0.622222222222
Exploring

New Goal: 2
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.93505
actor_epsilon 2: 0.572327044025
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.934975
actor_epsilon 1: 0.1


### EPISODE 644###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.26214015  0.30043158]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9349
actor_epsilon 6: 0.827586206897


### EPISODE 645###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.26530239  0.30359372]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.934825
actor_epsilon 4: 0.696551724138


### EPISODE 646###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.93475
actor_epsilon 1: 0.1


### EPISODE 647###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.27795097  0.31624249]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.934675
actor_epsilon 2: 0.575


### EPISODE 648###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.28110975  0.31940472]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.27794775  0.32256696]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9346
actor_epsilon 4: 0.691780821918
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.27478561  0.32572916]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.2716234   0.32889137]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.2684612   0.33205357]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.26213673  0.33837795]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.14597917  0.45778081]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.934525
actor_epsilon 2: 0.571428571429
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.25581226  0.34470221]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.93445
actor_epsilon 6: 0.828767123288


### EPISODE 649###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.934375
actor_epsilon 1: 0.1


### EPISODE 650###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.24948779  0.34470284]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9343
actor_epsilon 5: 0.756944444444


### EPISODE 651###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.24632555  0.34154075]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.934225
actor_epsilon 6: 0.829931972789


### EPISODE 652###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.92012018  0.57660162]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.93415
actor_epsilon 2: 0.567901234568
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.934075
actor_epsilon 2: 0.570552147239


### EPISODE 653###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.934
actor_epsilon 1: 0.1


### EPISODE 654###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.933925
actor_epsilon 6: 0.831081081081


### EPISODE 655###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93385
actor_epsilon 3: 0.625


### EPISODE 656###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.22418991  0.31940532]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.933775
actor_epsilon 4: 0.69387755102


### EPISODE 657###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.22102767  0.31624308]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.21786544  0.31308082]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9337
actor_epsilon 4: 0.689189189189
Exploring

New Goal: 2
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.11962914  0.77164894]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.933625
actor_epsilon 2: 0.567073170732
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.20837873  0.30359402]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93355
actor_epsilon 4: 0.691275167785


### EPISODE 658###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.2052165   0.30043176]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.19572979  0.29094496]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.933475
actor_epsilon 4: 0.686666666667
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.30370647  0.35225341]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18940529  0.28462043]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.27297866  0.33339247]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18308079  0.2782959 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.17675629  0.27197137]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.17359404  0.26880911]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16726954  0.26248458]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.16410729  0.25932232]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.16094504  0.25616005]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9334
actor_epsilon 6: 0.832214765101


### EPISODE 659###

New Goal: 2
State-Actions: 
Here ------> [[ 0.15778279  0.25299779]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.933325
actor_epsilon 2: 0.569696969697


### EPISODE 660###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.15462054  0.24983552]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.15145829  0.24667326]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14829604  0.24351099]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93325
actor_epsilon 5: 0.758620689655


### EPISODE 661###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.933175
actor_epsilon 6: 0.833333333333


### EPISODE 662###

New Goal: 2
State-Actions: 
Here ------> [[ 0.14197154  0.23718646]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9331
actor_epsilon 2: 0.572289156627


### EPISODE 663###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13880929  0.2340242 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.933025
actor_epsilon 5: 0.760273972603


### EPISODE 664###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93295
actor_epsilon 4: 0.688741721854


### EPISODE 665###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.13248481  0.22769967]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.932875
actor_epsilon 6: 0.834437086093


### EPISODE 666###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.12932257  0.2245374 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12616035  0.22137514]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.12299814  0.21821287]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11983595  0.21505061]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.11667379  0.21188834]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.1103519   0.20556384]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11351393  0.20240159]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.11667607  0.19923934]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9328
actor_epsilon 6: 0.835526315789


### EPISODE 667###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12300042  0.19291484]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.12932481  0.18659037]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.932725
actor_epsilon 6: 0.830065359477
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.132487    0.18342814]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.13564919  0.18026592]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.13881138  0.17710371]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14513573  0.17077945]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.93265
actor_epsilon 6: 0.831168831169


### EPISODE 668###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.932575
actor_epsilon 1: 0.1


### EPISODE 669###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.15146007  0.17078097]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.95066369  0.67461193]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9325
actor_epsilon 2: 0.568862275449

New Goal: 2
State-Actions: 
Here ------> [[ 0.15778439  0.17710537]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.932425
actor_epsilon 2: 0.565476190476
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93235
actor_epsilon 5: 0.761904761905


### EPISODE 670###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.932275
actor_epsilon 1: 0.1


### EPISODE 671###

New Goal: 2
State-Actions: 
Here ------> [[ 0.17043288  0.18975434]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9322
actor_epsilon 2: 0.562130177515
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.17675716  0.19607884]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.932125
actor_epsilon 6: 0.832258064516


### EPISODE 672###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.17991932  0.1992411 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.18624368  0.20556563]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18940587  0.2087279 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.19256809  0.21189016]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.19573031  0.21505243]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.93205
actor_epsilon 6: 0.826923076923
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.19889255  0.21821469]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.931975
actor_epsilon 5: 0.756756756757
Exploring

New Goal: 1
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.20521702  0.22453922]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.9319
actor_epsilon 1: 0.1


### EPISODE 673###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.931825
actor_epsilon 1: 0.1


### EPISODE 674###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.21786596  0.23718828]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22102818  0.24035054]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.2241904   0.24351281]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93175
actor_epsilon 4: 0.690789473684


### EPISODE 675###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.931675
actor_epsilon 4: 0.692810457516


### EPISODE 676###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.23051472  0.24983734]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.23367643  0.2529996 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.23051462  0.25616187]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22735253  0.25932413]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.22419038  0.2624864 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22102821  0.26564866]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.21470383  0.27197319]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.21154164  0.27513546]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.20837946  0.27829772]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.20521732  0.28145999]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.20205523  0.28462225]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.19256942  0.29410905]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18940748  0.29727131]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.18624555  0.30043358]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.18308355  0.30359584]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.9316
actor_epsilon 6: 0.821656050955
Exploring

New Goal: 2
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16727281  0.31940717]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.16411059  0.32256943]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.16094837  0.32573169]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.1546239   0.33205622]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.931525
actor_epsilon 2: 0.558823529412
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.93145
actor_epsilon 3: 0.627737226277


### EPISODE 677###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.14513719  0.34154302]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.931375
actor_epsilon 5: 0.758389261745


### EPISODE 678###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.14197496  0.34470528]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9313
actor_epsilon 5: 0.76


### EPISODE 679###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.931225
actor_epsilon 1: 0.1


### EPISODE 680###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12932606  0.35735434]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.93115
actor_epsilon 3: 0.623188405797
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.931075
actor_epsilon 1: 0.1


### EPISODE 681###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.931
actor_epsilon 3: 0.625899280576


### EPISODE 682###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.12300076  0.37632793]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.930925
actor_epsilon 2: 0.555555555556
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.93085
actor_epsilon 1: 0.1


### EPISODE 683###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.930775
actor_epsilon 2: 0.558139534884


### EPISODE 684###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9307
actor_epsilon 4: 0.694805194805


### EPISODE 685###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.13881162  0.39213926]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.930625
actor_epsilon 6: 0.822784810127


### EPISODE 686###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.93055
actor_epsilon 1: 0.1


### EPISODE 687###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.14513604  0.39846373]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.930475
actor_epsilon 5: 0.761589403974


### EPISODE 688###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9304
actor_epsilon 3: 0.628571428571


### EPISODE 689###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.15146048  0.40478805]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.930325
actor_epsilon 4: 0.696774193548


### EPISODE 690###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.93025
actor_epsilon 4: 0.698717948718


### EPISODE 691###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.930175
actor_epsilon 1: 0.1


### EPISODE 692###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.16094714  0.39530161]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.16410936  0.39213938]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.49470383  0.52341664]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.46851176  0.49664208]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.17675824  0.37949032]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.9301
actor_epsilon 6: 0.817610062893
Exploring

New Goal: 1
State-Actions: 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24100699  0.35467988]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.930025
actor_epsilon 1: 0.1


### EPISODE 693###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92995
actor_epsilon 1: 0.1


### EPISODE 694###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.21786733  0.33838087]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22102958  0.33521861]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.22419183  0.33205634]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22735408  0.32889408]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.59751904  0.33494604]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.23367856  0.32256955]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24000303  0.31624502]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.24316525  0.31308275]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.929875
actor_epsilon 6: 0.81875


### EPISODE 695###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.24632747  0.30992049]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9298
actor_epsilon 2: 0.554913294798
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.929725
actor_epsilon 5: 0.763157894737


### EPISODE 696###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.25581405  0.3004337 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92965
actor_epsilon 5: 0.764705882353


### EPISODE 697###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.929575
actor_epsilon 1: 0.1


### EPISODE 698###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.26213825  0.29410917]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.26213828  0.28778464]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.25897625  0.28462237]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.9295
actor_epsilon 4: 0.694267515924
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.25581414  0.28146011]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.24948978  0.27513558]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.929425
actor_epsilon 2: 0.551724137931
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 1.0 True
Goal reached!! 
meta_epsilon: 0.92935
actor_epsilon 1: 0.1


### EPISODE 699###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.81880081  0.77792615]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.929275
actor_epsilon 1: 0.1

[[ 0.7    1.031  0.536  0.315  0.184  0.074]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 700###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.23051658  0.25616199]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22735438  0.25299972]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.6425122   0.62740386]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22102994  0.24667519]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.21786772  0.24351293]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9292
actor_epsilon 6: 0.819875776398


### EPISODE 701###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.929125
actor_epsilon 1: 0.1


### EPISODE 702###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.04834056  0.66737127]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92905
actor_epsilon 1: 0.1


### EPISODE 703###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.00017846  0.62485373]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.928975
actor_epsilon 1: 0.1


### EPISODE 704###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9289
actor_epsilon 2: 0.554285714286


### EPISODE 705###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.19573201  0.22137707]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.928825
actor_epsilon 3: 0.624113475177

New Goal: 2
State-Actions: 
Here ------> [[ 0.95405054  0.54240626]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.92875
actor_epsilon 2: 0.551136363636
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.18940751  0.21505255]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.928675
actor_epsilon 3: 0.619718309859
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.18624526  0.21189028]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.48847789  0.28425533]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.17992076  0.20556575]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.17675851  0.20240349]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9286
actor_epsilon 6: 0.820987654321


### EPISODE 706###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.17359626  0.19924124]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.17043401  0.19607899]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.928525
actor_epsilon 6: 0.822085889571


### EPISODE 707###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.16410951  0.18975453]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92845
actor_epsilon 6: 0.823170731707


### EPISODE 708###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.928375
actor_epsilon 1: 0.1


### EPISODE 709###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.15778501  0.18343106]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.15462276  0.18659323]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.15146051  0.18975545]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9283
actor_epsilon 6: 0.824242424242


### EPISODE 710###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.14829826  0.19291769]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.928225
actor_epsilon 3: 0.622377622378


### EPISODE 711###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92815
actor_epsilon 3: 0.625


### EPISODE 712###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.928075
actor_epsilon 2: 0.553672316384


### EPISODE 713###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13881151  0.20240444]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.13564926  0.2055667 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.928
actor_epsilon 5: 0.766233766234


### EPISODE 714###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.927925
actor_epsilon 4: 0.696202531646


### EPISODE 715###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12616251  0.2150535 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92785
actor_epsilon 3: 0.627586206897


### EPISODE 716###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.12300026  0.21821576]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11983801  0.22137803]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11351353  0.22770256]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.927775
actor_epsilon 4: 0.691823899371

New Goal: 2
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.09181035  1.13473916]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9277
actor_epsilon 2: 0.550561797753
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.927625
actor_epsilon 5: 0.767741935484


### EPISODE 717###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.73120487  0.82111108]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92755
actor_epsilon 1: 0.1


### EPISODE 718###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.09770239  0.24351388]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.927475
actor_epsilon 3: 0.630136986301


### EPISODE 719###

New Goal: 2
State-Actions: 
Here ------> [[ 0.09454019  0.24667615]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9274
actor_epsilon 2: 0.54748603352
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.08821581  0.25300068]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.927325
actor_epsilon 3: 0.625850340136
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.08505364  0.25616294]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.08189149  0.25932521]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.92725
actor_epsilon 5: 0.762820512821
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.07872937  0.26248747]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.07556729  0.26564974]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.07240527  0.268812  ]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.927175
actor_epsilon 2: 0.544444444444
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.0755673  0.2782988]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9271
actor_epsilon 2: 0.541436464088
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08189158  0.28462332]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08505378  0.28778559]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.088216    0.29094785]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09137823  0.29411012]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.09454046  0.29727238]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.0977027   0.30043465]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.10086494  0.30359691]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.10718942  0.30992144]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12997608  0.31375924]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.11351391  0.31624597]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.927025
actor_epsilon 6: 0.825301204819


### EPISODE 720###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.14323127  0.31826869]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92695
actor_epsilon 2: 0.543956043956


### EPISODE 721###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.14936194  0.32248235]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.15516058  0.3270326 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.926875
actor_epsilon 5: 0.764331210191


### EPISODE 722###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.16575941  0.33714506]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.1750233   0.34861067]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9268
actor_epsilon 5: 0.76582278481


### EPISODE 723###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.21120934  0.36542735]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.926725
actor_epsilon 4: 0.69375


### EPISODE 724###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92665
actor_epsilon 1: 0.1


### EPISODE 725###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.18640614  0.36835617]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.19503585  0.37561947]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.2040088   0.38322449]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.926575
actor_epsilon 5: 0.767295597484


### EPISODE 726###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9265
actor_epsilon 1: 0.1


### EPISODE 727###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.50092888  0.57327676]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.926425
actor_epsilon 1: 0.1


### EPISODE 728###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92635
actor_epsilon 3: 0.628378378378


### EPISODE 729###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.24793939  0.4108949 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24759102  0.41028002]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.926275
actor_epsilon 5: 0.76875


### EPISODE 730###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9262
actor_epsilon 1: 0.1


### EPISODE 731###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.24622217  0.40179026]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.926125
actor_epsilon 5: 0.770186335404


### EPISODE 732###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92605
actor_epsilon 5: 0.771604938272


### EPISODE 733###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.29934171  0.39379191]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.26095295  0.38017678]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.925975
actor_epsilon 4: 0.689440993789
Exploring

New Goal: 3
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9259
actor_epsilon 3: 0.624161073826
Exploring

New Goal: 1
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.99291122  0.27640587]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.925825
actor_epsilon 1: 0.1


### EPISODE 734###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.35627267  0.3402462 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92575
actor_epsilon 4: 0.691358024691


### EPISODE 735###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.30008513  0.34311035]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.41988719  0.18129806]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.925675
actor_epsilon 2: 0.540983606557
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9256
actor_epsilon 3: 0.626666666667


### EPISODE 736###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.925525
actor_epsilon 1: 0.1


### EPISODE 737###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.30524498  0.34879413]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.92545
actor_epsilon 4: 0.687116564417
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 1.30265021  0.47062066]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.925375
actor_epsilon 3: 0.622516556291
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.29965302  0.35066801]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.29488137  0.3517071 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.33737025  0.37398273]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9253
actor_epsilon 4: 0.689024390244


### EPISODE 738###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.925225
actor_epsilon 6: 0.826347305389


### EPISODE 739###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.35730508  0.38196108]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92515
actor_epsilon 4: 0.690909090909


### EPISODE 740###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.30922258  0.35126042]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.925075
actor_epsilon 2: 0.54347826087


### EPISODE 741###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.925
actor_epsilon 1: 0.1


### EPISODE 742###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.25581864  0.31308341]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.924925
actor_epsilon 3: 0.625


### EPISODE 743###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.3100068   0.34798834]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.30765426  0.34639424]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.24633247  0.30359662]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.30219153  0.34244978]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.29370373  0.34009907]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.28546616  0.33237004]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.23368357  0.29094756]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.2697424   0.31766412]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.26114684  0.3098588 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.22103463  0.2782985 ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.92485
actor_epsilon 5: 0.766871165644
Exploring

New Goal: 1
State-Actions: 
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 1.03361905  0.40801165]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.924775
actor_epsilon 1: 0.1


### EPISODE 744###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9247
actor_epsilon 1: 0.1


### EPISODE 745###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.924625
actor_epsilon 4: 0.692771084337


### EPISODE 746###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.19257444  0.24983813]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.92455
actor_epsilon 3: 0.62091503268
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.4055382   0.26272449]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.924475
actor_epsilon 3: 0.623376623377


### EPISODE 747###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9244
actor_epsilon 5: 0.768292682927


### EPISODE 748###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.17992544  0.23718925]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.924325
actor_epsilon 3: 0.61935483871
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.17676319  0.23402719]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.92425
actor_epsilon 4: 0.688622754491
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.17360094  0.23718914]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 1.23589873  0.62483549]]
(5, 0); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.924175
actor_epsilon 4: 0.684523809524
Exploring

New Goal: 2
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.05776513  0.6168164 ]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9241
actor_epsilon 2: 0.540540540541
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.924025
actor_epsilon 3: 0.621794871795


### EPISODE 749###

New Goal: 2
State-Actions: 
Here ------> [[ 0.15778969  0.25300026]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92395
actor_epsilon 2: 0.543010752688


### EPISODE 750###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.923875
actor_epsilon 1: 0.1


### EPISODE 751###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.01523495  0.72807479]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9238
actor_epsilon 1: 0.1


### EPISODE 752###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.923725
actor_epsilon 1: 0.1


### EPISODE 753###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92365
actor_epsilon 1: 0.1


### EPISODE 754###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.923575
actor_epsilon 1: 0.1


### EPISODE 755###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.12671687  0.284958  ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.12300494  0.28778517]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9235
actor_epsilon 4: 0.686390532544


### EPISODE 756###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.11668044  0.2941097 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.10086923  0.30992103]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.097707    0.31308329]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.923425
actor_epsilon 5: 0.769696969697


### EPISODE 757###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92335
actor_epsilon 1: 0.1


### EPISODE 758###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.923275
actor_epsilon 2: 0.545454545455


### EPISODE 759###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9232
actor_epsilon 2: 0.547872340426


### EPISODE 760###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.0850582   0.32573235]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.923125
actor_epsilon 6: 0.827380952381


### EPISODE 761###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.0818961   0.32889462]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92305
actor_epsilon 4: 0.688235294118


### EPISODE 762###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.922975
actor_epsilon 3: 0.624203821656


### EPISODE 763###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9229
actor_epsilon 2: 0.550264550265


### EPISODE 764###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08505826  0.33838141]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.922825
actor_epsilon 4: 0.690058479532


### EPISODE 765###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09138262  0.34470594]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.09454483  0.3478682 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.10086929  0.3541927 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92275
actor_epsilon 5: 0.771084337349


### EPISODE 766###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.922675
actor_epsilon 2: 0.552631578947


### EPISODE 767###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.10719376  0.36051717]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9226
actor_epsilon 6: 0.828402366864


### EPISODE 768###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.110356    0.36367941]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11351825  0.36684164]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.11668049  0.37000388]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.11984273  0.37316611]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.922525
actor_epsilon 6: 0.829411764706


### EPISODE 769###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.12616719  0.37949058]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.92245
actor_epsilon 3: 0.620253164557
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 1.27358902  0.55026352]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.922375
actor_epsilon 2: 0.549738219895

New Goal: 2
State-Actions: 
Here ------> [[ 0.13249163  0.38581505]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9223
actor_epsilon 2: 0.552083333333


### EPISODE 770###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13565385  0.38897726]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.922225
actor_epsilon 5: 0.77245508982


### EPISODE 771###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.13881606  0.39213946]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.92215
actor_epsilon 3: 0.616352201258
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.922075
actor_epsilon 1: 0.1


### EPISODE 772###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.922
actor_epsilon 2: 0.554404145078


### EPISODE 773###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.921925
actor_epsilon 6: 0.830409356725


### EPISODE 774###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.15462691  0.38896284]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92185
actor_epsilon 3: 0.61875


### EPISODE 775###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.15778908  0.3858006 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.16095129  0.38263837]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16411349  0.37947613]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.16727571  0.37631389]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.921775
actor_epsilon 6: 0.825581395349
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.17043793  0.37315166]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.9217
actor_epsilon 5: 0.767857142857
Exploring

New Goal: 5
State-Actions: 
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.1767624   0.36682713]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.18308689  0.3605026 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.18624914  0.35734034]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.18941139  0.35417807]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Goal reached!! 
meta_epsilon: 0.921625
actor_epsilon 5: 0.763313609467
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.19257364  0.35101581]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 1.53818023  1.12098789]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.92155
actor_epsilon 3: 0.614906832298
Exploring

New Goal: 6
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.20206039  0.34152901]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.921475
actor_epsilon 6: 0.826589595376


### EPISODE 776###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.20522264  0.33836675]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.20838489  0.33520448]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.21154714  0.33204222]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9214
actor_epsilon 6: 0.827586206897


### EPISODE 777###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.921325
actor_epsilon 5: 0.764705882353


### EPISODE 778###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92125
actor_epsilon 4: 0.691860465116


### EPISODE 779###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.22103389  0.32255542]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22419614  0.31939316]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.22735839  0.31623089]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.921175
actor_epsilon 4: 0.693641618497


### EPISODE 780###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.23052064  0.31306863]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9211
actor_epsilon 3: 0.617283950617


### EPISODE 781###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.0793097   0.80575168]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.921025
actor_epsilon 2: 0.551546391753
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.24000736  0.30358183]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92095
actor_epsilon 6: 0.828571428571


### EPISODE 782###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.920875
actor_epsilon 4: 0.695402298851


### EPISODE 783###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9208
actor_epsilon 1: 0.1


### EPISODE 784###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.24949403  0.29409504]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.920725
actor_epsilon 4: 0.697142857143


### EPISODE 785###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92065
actor_epsilon 6: 0.829545454545


### EPISODE 786###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.25581846  0.28777051]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.920575
actor_epsilon 3: 0.613496932515
Exploring

New Goal: 3
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9205
actor_epsilon 3: 0.615853658537


### EPISODE 787###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.920425
actor_epsilon 5: 0.766081871345


### EPISODE 788###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.83154511  0.56176949]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.92035
actor_epsilon 1: 0.1


### EPISODE 789###

New Goal: 2
State-Actions: 
Here ------> [[ 0.27162942  0.27195919]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.920275
actor_epsilon 2: 0.553846153846


### EPISODE 790###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.2747916   0.26879692]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9202
actor_epsilon 2: 0.55612244898


### EPISODE 791###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.920125
actor_epsilon 1: 0.1


### EPISODE 792###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.28744021  0.25614786]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.92005
actor_epsilon 3: 0.618181818182


### EPISODE 793###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.29060236  0.25298563]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919975
actor_epsilon 5: 0.767441860465


### EPISODE 794###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.29376453  0.24982339]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9199
actor_epsilon 5: 0.768786127168


### EPISODE 795###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.29692671  0.24666117]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919825
actor_epsilon 4: 0.698863636364


### EPISODE 796###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91975
actor_epsilon 4: 0.700564971751


### EPISODE 797###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919675
actor_epsilon 3: 0.620481927711


### EPISODE 798###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9196
actor_epsilon 5: 0.770114942529


### EPISODE 799###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.30957538  0.24034551]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919525
actor_epsilon 5: 0.771428571429

[[ 0.8    1.176  0.597  0.339  0.194  0.076]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 800###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.31273746  0.24350771]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91945
actor_epsilon 4: 0.702247191011


### EPISODE 801###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919375
actor_epsilon 3: 0.622754491018


### EPISODE 802###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.31274059  0.2498322 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9193
actor_epsilon 6: 0.830508474576


### EPISODE 803###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.919225
actor_epsilon 1: 0.1


### EPISODE 804###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.30641645  0.25615671]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91915
actor_epsilon 3: 0.625


### EPISODE 805###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.30325425  0.25931898]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919075
actor_epsilon 3: 0.627218934911


### EPISODE 806###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.30009201  0.26248124]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.919
actor_epsilon 2: 0.558375634518


### EPISODE 807###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.29692978  0.26564351]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.918925
actor_epsilon 6: 0.831460674157


### EPISODE 808###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.29376754  0.26880577]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91885
actor_epsilon 4: 0.703910614525


### EPISODE 809###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.29060531  0.27196804]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.918775
actor_epsilon 4: 0.705555555556


### EPISODE 810###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9187
actor_epsilon 2: 0.560606060606


### EPISODE 811###

New Goal: 2
State-Actions: 
Here ------> [[ 0.28428078  0.27829257]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.918625
actor_epsilon 2: 0.562814070352


### EPISODE 812###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.27795625  0.2846171 ]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.27479398  0.28777936]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91855
actor_epsilon 5: 0.772727272727


### EPISODE 813###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.27163172  0.29094163]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.918475
actor_epsilon 2: 0.565


### EPISODE 814###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9184
actor_epsilon 1: 0.1


### EPISODE 815###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.918325
actor_epsilon 6: 0.832402234637


### EPISODE 816###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91825
actor_epsilon 2: 0.567164179104


### EPISODE 817###
Exploring

New Goal: 5
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.918175
actor_epsilon 5: 0.774011299435


### EPISODE 818###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.25582039  0.30675295]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9181
actor_epsilon 5: 0.775280898876


### EPISODE 819###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.25265813  0.30991521]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24949586  0.31307748]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.2463336   0.31623974]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.24317133  0.31940201]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.24000907  0.32256427]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.2368468   0.32572654]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.23052227  0.33205107]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.22736001  0.33521333]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.22419775  0.3383756 ]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.918025
actor_epsilon 6: 0.827777777778
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.22103548  0.34153786]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.21787322  0.34470013]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.19889963  0.36367372]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.1925751   0.36999825]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.18941283  0.37316051]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.18625057  0.37632278]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.91795
actor_epsilon 3: 0.623529411765
Exploring

New Goal: 3
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 1.82956994  0.66569245]]
(4, 0); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.917875
actor_epsilon 3: 0.619883040936
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 1.48478699  0.65640855]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9178
actor_epsilon 2: 0.564356435644
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.917725
actor_epsilon 4: 0.707182320442


### EPISODE 820###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.16095245  0.40162084]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91765
actor_epsilon 6: 0.828729281768


### EPISODE 821###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.917575
actor_epsilon 4: 0.708791208791


### EPISODE 822###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.15462792  0.40794528]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9175
actor_epsilon 3: 0.622093023256


### EPISODE 823###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.917425
actor_epsilon 2: 0.566502463054


### EPISODE 824###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.14830339  0.41426948]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91735
actor_epsilon 6: 0.82967032967


### EPISODE 825###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.917275
actor_epsilon 1: 0.1


### EPISODE 826###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9172
actor_epsilon 1: 0.1


### EPISODE 827###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.917125
actor_epsilon 2: 0.56862745098


### EPISODE 828###

New Goal: 2
State-Actions: 
Here ------> [[ 0.11668074  0.38264769]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.11035621  0.37632319]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.10719395  0.37316093]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.10403169  0.36999866]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.10086943  0.3668364 ]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.09454492  0.36051187]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.09138267  0.3573496 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.91705
actor_epsilon 2: 0.565853658537
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08505817  0.35102507]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.916975
actor_epsilon 6: 0.830601092896


### EPISODE 829###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9169
actor_epsilon 3: 0.624277456647


### EPISODE 830###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.07873367  0.34470055]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.916825
actor_epsilon 6: 0.83152173913


### EPISODE 831###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.23317683  0.37752789]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07240918  0.33837602]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.91675
actor_epsilon 4: 0.704918032787
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.06924694  0.33521375]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.0660847   0.33205149]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.06292245  0.32888922]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.916675
actor_epsilon 5: 0.776536312849


### EPISODE 832###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9166
actor_epsilon 1: 0.1


### EPISODE 833###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.05659795  0.32256469]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.916525
actor_epsilon 2: 0.567961165049


### EPISODE 834###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.0534357   0.31940243]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.91645
actor_epsilon 3: 0.620689655172
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.916375
actor_epsilon 1: 0.1


### EPISODE 835###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.25410628  0.76451135]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9163
actor_epsilon 2: 0.565217391304

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.916225
actor_epsilon 2: 0.567307692308


### EPISODE 836###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.10644889  0.62556505]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.91615
actor_epsilon 2: 0.564593301435
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.02813767  0.29410431]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.02497542  0.29094204]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.01548867  0.28145528]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.01232644  0.27829304]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.916075
actor_epsilon 5: 0.777777777778


### EPISODE 837###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.00600204  0.27196857]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.916
actor_epsilon 5: 0.779005524862


### EPISODE 838###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.915925
actor_epsilon 1: 0.1


### EPISODE 839###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.00600199  0.26564416]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.91585
actor_epsilon 3: 0.617142857143
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.00916415  0.26248202]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.915775
actor_epsilon 4: 0.701086956522
Exploring

New Goal: 1
State-Actions: 
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.84553927  0.8024891 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9157
actor_epsilon 1: 0.1


### EPISODE 840###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.915625
actor_epsilon 2: 0.566666666667


### EPISODE 841###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.02497532  0.25932127]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.03129983  0.26564556]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.04815991  0.27397561]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.03762434  0.27197   ]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.04394885  0.27829447]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.04711111  0.28145671]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.05027337  0.28461897]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05343562  0.28778124]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.05659788  0.2909435 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05976014  0.29410577]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.0629224   0.29726803]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.06608465  0.3004303 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07240917  0.30675483]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.07557142  0.30991709]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.07873367  0.31307936]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.08189592  0.31624162]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.08505817  0.31940389]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.09138267  0.32572842]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.09454492  0.32889068]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.09770718  0.33205295]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.91555
actor_epsilon 6: 0.827027027027
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.10086944  0.33521521]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.01893544  1.0637989 ]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.11351848  0.34786427]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.11984301  0.3541888 ]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.12616754  0.36051333]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.99229306  0.89111829]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.915475
actor_epsilon 2: 0.563981042654
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13249207  0.36683786]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.9154
actor_epsilon 5: 0.78021978022


### EPISODE 842###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.13565433  0.37000012]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.915325
actor_epsilon 5: 0.781420765027


### EPISODE 843###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.1388166   0.37316239]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91525
actor_epsilon 4: 0.702702702703


### EPISODE 844###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.915175
actor_epsilon 3: 0.613636363636
Exploring

New Goal: 3
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.14830339  0.38264918]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9151
actor_epsilon 3: 0.61581920904


### EPISODE 845###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.915025
actor_epsilon 2: 0.566037735849


### EPISODE 846###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91495
actor_epsilon 4: 0.704301075269


### EPISODE 847###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.31628531  0.43151191]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.914875
actor_epsilon 6: 0.827956989247


### EPISODE 848###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.16095245  0.39529824]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9148
actor_epsilon 3: 0.612359550562
Exploring

New Goal: 1
State-Actions: 
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 1.0709883   0.54737484]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.914725
actor_epsilon 1: 0.1


### EPISODE 849###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.39824855  0.43551031]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91465
actor_epsilon 6: 0.828877005348


### EPISODE 850###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.17360151  0.40794727]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.914575
actor_epsilon 4: 0.705882352941


### EPISODE 851###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9145
actor_epsilon 1: 0.1


### EPISODE 852###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.17992604  0.41427174]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.914425
actor_epsilon 2: 0.568075117371


### EPISODE 853###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.1830883   0.41743398]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91435
actor_epsilon 4: 0.707446808511


### EPISODE 854###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.914275
actor_epsilon 1: 0.1


### EPISODE 855###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.18941283  0.42375845]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9142
actor_epsilon 5: 0.782608695652


### EPISODE 856###
Exploring

New Goal: 4
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.19573736  0.43008289]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Goal reached!! 
meta_epsilon: 0.914125
actor_epsilon 4: 0.703703703704
Exploring

New Goal: 1
State-Actions: 
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.68952113  0.37965396]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.91405
actor_epsilon 1: 0.1


### EPISODE 857###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.913975
actor_epsilon 2: 0.570093457944


### EPISODE 858###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9139
actor_epsilon 1: 0.1


### EPISODE 859###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.21471095  0.44273528]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.21787322  0.43957329]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.22103548  0.4364112 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.913825
actor_epsilon 5: 0.783783783784


### EPISODE 860###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.23052227  0.42692471]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.23368454  0.4237625 ]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.2368468  0.4206003]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.24000907  0.41743806]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.24317133  0.41427583]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.24949586  0.40795135]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.25265813  0.40478912]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.25582039  0.40162688]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.25898266  0.39846465]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.91375
actor_epsilon 6: 0.824468085106
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.26214492  0.39530241]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.26530719  0.39214018]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.913675
actor_epsilon 6: 0.820105820106
Exploring

New Goal: 6
State-Actions: 
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.27163172  0.38581568]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.27479398  0.38265342]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.28111851  0.37632889]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.28428078  0.37316662]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.28744304  0.37000436]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.29060531  0.36684209]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.29376757  0.36367983]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.29692984  0.36051756]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.31598529  0.36752203]]
(2, 1); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.9136
actor_epsilon 6: 0.821052631579


### EPISODE 861###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.913525
actor_epsilon 2: 0.572093023256


### EPISODE 862###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91345
actor_epsilon 3: 0.614525139665


### EPISODE 863###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.3095789  0.3478685]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.913375
actor_epsilon 2: 0.574074074074


### EPISODE 864###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.31274116  0.34470624]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9133
actor_epsilon 2: 0.576036866359


### EPISODE 865###
Exploring

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.95811635  0.93784755]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.913225
actor_epsilon 2: 0.573394495413
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.32222795  0.33521944]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91315
actor_epsilon 4: 0.705263157895


### EPISODE 866###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.913075
actor_epsilon 1: 0.1


### EPISODE 867###

New Goal: 2
State-Actions: 
Here ------> [[ 0.32855248  0.32889491]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.913
actor_epsilon 2: 0.575342465753


### EPISODE 868###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.33171475  0.32573265]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.912925
actor_epsilon 4: 0.706806282723


### EPISODE 869###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.95288175  1.16122627]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.91285
actor_epsilon 2: 0.572727272727
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.912775
actor_epsilon 4: 0.708333333333


### EPISODE 870###

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9127
actor_epsilon 2: 0.574660633484


### EPISODE 871###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.3538506  0.303597 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.912625
actor_epsilon 5: 0.784946236559


### EPISODE 872###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.35701287  0.30043477]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91255
actor_epsilon 3: 0.616666666667


### EPISODE 873###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.6788528   0.62835622]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.912475
actor_epsilon 1: 0.1


### EPISODE 874###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.36966193  0.28778583]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9124
actor_epsilon 4: 0.709844559585


### EPISODE 875###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.912325
actor_epsilon 1: 0.1


### EPISODE 876###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91225
actor_epsilon 4: 0.711340206186


### EPISODE 877###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.912175
actor_epsilon 1: 0.1


### EPISODE 878###
Exploring

New Goal: 3
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.9121
actor_epsilon 3: 0.613259668508
Exploring

New Goal: 5
State-Actions: 
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.4076089  0.2751359]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.912025
actor_epsilon 5: 0.786096256684


### EPISODE 879###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.4107711   0.27829814]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91195
actor_epsilon 5: 0.787234042553


### EPISODE 880###
Exploring

New Goal: 1
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.911875
actor_epsilon 1: 0.1


### EPISODE 881###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.9118
actor_epsilon 1: 0.1


### EPISODE 882###

New Goal: 2
State-Actions: 
Here ------> [[ 0.42658094  0.29410946]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.911725
actor_epsilon 2: 0.576576576577


### EPISODE 883###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.42341921  0.29727173]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91165
actor_epsilon 2: 0.578475336323


### EPISODE 884###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.19750357  0.52542448]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.911575
actor_epsilon 2: 0.575892857143
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9115
actor_epsilon 3: 0.615384615385


### EPISODE 885###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.41077065  0.30992079]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.911425
actor_epsilon 2: 0.577777777778


### EPISODE 886###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.40760845  0.31308305]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91135
actor_epsilon 2: 0.579646017699


### EPISODE 887###

New Goal: 2
State-Actions: 
Here ------> [[ 0.40444621  0.31624532]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.911275
actor_epsilon 2: 0.581497797357


### EPISODE 888###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.41754237  0.32180646]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9112
actor_epsilon 6: 0.821989528796


### EPISODE 889###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.39812174  0.32256985]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.911125
actor_epsilon 3: 0.617486338798


### EPISODE 890###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.39495948  0.32573211]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91105
actor_epsilon 2: 0.583333333333


### EPISODE 891###
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 1.32654285  0.40241933]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.910975
actor_epsilon 1: 0.1


### EPISODE 892###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.38863495  0.33205664]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9109
actor_epsilon 4: 0.712820512821


### EPISODE 893###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.53174663  0.340653  ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.910825
actor_epsilon 6: 0.822916666667


### EPISODE 894###

New Goal: 2
State-Actions: 
Here ------> [[ 0.38231042  0.33838117]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91075
actor_epsilon 2: 0.585152838428


### EPISODE 895###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.37914816  0.34154344]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.910675
actor_epsilon 4: 0.714285714286


### EPISODE 896###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9106
actor_epsilon 5: 0.78835978836


### EPISODE 897###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.910525
actor_epsilon 3: 0.619565217391


### EPISODE 898###
Exploring

New Goal: 4
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91045
actor_epsilon 4: 0.715736040609


### EPISODE 899###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.910375
actor_epsilon 2: 0.582608695652
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.36017457  0.36051702]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9103
actor_epsilon 3: 0.621621621622

[[ 0.9    1.306  0.649  0.375  0.22   0.088]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]
 [ 0.     0.     0.     0.     0.     0.   ]]


### EPISODE 900###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.3570123   0.36367929]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.910225
actor_epsilon 6: 0.823834196891


### EPISODE 901###
Exploring

New Goal: 3
State-Actions: 
Here ------> [[ 0.35385004  0.36684155]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Goal reached!! 
meta_epsilon: 0.91015
actor_epsilon 3: 0.618279569892
Exploring

New Goal: 2
State-Actions: 
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.34752551  0.37316608]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.79600072  0.5261752 ]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.910075
actor_epsilon 2: 0.580086580087
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.91
actor_epsilon 5: 0.789473684211


### EPISODE 902###
Exploring

New Goal: 6
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.909925
actor_epsilon 6: 0.824742268041


### EPISODE 903###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.33487645  0.38581514]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.33171418  0.38897741]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.32855192  0.39213967]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.32538965  0.39530194]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.32222739  0.3984642 ]]
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.31906512  0.40162647]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.39507335  0.44098133]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.90985
actor_epsilon 6: 0.825641025641


### EPISODE 904###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.909775
actor_epsilon 1: 0.1


### EPISODE 905###
Exploring

New Goal: 2
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9097
actor_epsilon 2: 0.581896551724


### EPISODE 906###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.30009153  0.42060006]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.29692927  0.42376232]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.909625
actor_epsilon 5: 0.79057591623


### EPISODE 907###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.29060474  0.43008685]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.90955
actor_epsilon 6: 0.826530612245


### EPISODE 908###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.28744248  0.43324912]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.909475
actor_epsilon 4: 0.717171717172


### EPISODE 909###
Exploring

New Goal: 5
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9094
actor_epsilon 5: 0.791666666667


### EPISODE 910###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.28111795  0.43957365]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.909325
actor_epsilon 5: 0.79274611399


### EPISODE 911###

New Goal: 2
State-Actions: 
Here ------> [[ 0.27795568  0.44273591]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 1.03850782  1.08455527]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.90925
actor_epsilon 2: 0.579399141631
Exploring

New Goal: 1
State-Actions: 
Here ------> [[ 0.74220562  0.84885502]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.72828233  0.85233289]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.909175
actor_epsilon 1: 0.1


### EPISODE 912###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.2494953   0.47119629]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9091
actor_epsilon 2: 0.581196581197


### EPISODE 913###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.24633303  0.47435856]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.909025
actor_epsilon 2: 0.582978723404


### EPISODE 914###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.90895
actor_epsilon 6: 0.827411167513


### EPISODE 915###

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.908875
actor_epsilon 2: 0.584745762712


### EPISODE 916###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.23684624  0.48384535]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9088
actor_epsilon 2: 0.586497890295


### EPISODE 917###

New Goal: 2
State-Actions: 
Here ------> [[ 0.23368397  0.48700759]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.908725
actor_epsilon 2: 0.588235294118


### EPISODE 918###
Exploring

New Goal: 6
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.22735944  0.49333203]]
(3, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.22419718  0.49649373]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.22103491  0.49333155]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
Goal reached!! 
S6 reached!! 
meta_epsilon: 0.90865
actor_epsilon 6: 0.823232323232
Exploring

New Goal: 2
State-Actions: 
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.21471038  0.48700711]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.20838585  0.48068261]]
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.19889906  0.47119582]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.19257453  0.46487129]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.18941227  0.46170902]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.18625     0.45854676]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.17992547  0.45222223]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
Here ------> [[ 0.17043868  0.44273543]]
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
Here ------> [[ 0.16727641  0.43957317]]
(6, 1); 
next_state, external_reward, done 5 0.0 False
(5, 0); 
next_state, external_reward, done 4 0.0 False
Here ------> [[ 0.16095188  0.43324864]]
(4, 1); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 6 0.0 False
S6 reached!! 
(6, 0); 
next_state, external_reward, done 5 0.0 False
(5, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.75176346  0.52866995]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.908575
actor_epsilon 2: 0.585774058577

New Goal: 2
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.87754357  0.52414608]]
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.9085
actor_epsilon 2: 0.583333333333
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.13565376  0.40795052]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.12932923  0.40162599]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 1.0 True
meta_epsilon: 0.908425
actor_epsilon 4: 0.718592964824


### EPISODE 919###

New Goal: 2
State-Actions: 
Here ------> [[ 0.11984244  0.3921392 ]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
Goal reached!! 
meta_epsilon: 0.90835
actor_epsilon 2: 0.580912863071
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.11351791  0.38581467]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.908275
actor_epsilon 6: 0.824120603015


### EPISODE 920###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.11035565  0.3826524 ]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9082
actor_epsilon 2: 0.582644628099


### EPISODE 921###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.908125
actor_epsilon 3: 0.620320855615


### EPISODE 922###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 4 0.0 False
(4, 0); 
next_state, external_reward, done 3 0.0 False
(3, 0); 
next_state, external_reward, done 2 0.0 False
(2, 0); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.90805
actor_epsilon 1: 0.1


### EPISODE 923###
Exploring

New Goal: 4
State-Actions: 
Here ------> [[ 0.08821981  0.36051655]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.907975
actor_epsilon 4: 0.72


### EPISODE 924###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.08505756  0.35735428]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9079
actor_epsilon 6: 0.825


### EPISODE 925###
Exploring

New Goal: 2
State-Actions: 
Here ------> [[ 0.08189531  0.35419202]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.907825
actor_epsilon 2: 0.584362139918


### EPISODE 926###
Exploring

New Goal: 1
State-Actions: 
(2, 1); 
next_state, external_reward, done 1 0.01 True
Goal reached!! 
meta_epsilon: 0.90775
actor_epsilon 1: 0.1


### EPISODE 927###

New Goal: 2
State-Actions: 
Here ------> [[ 0.07557081  0.34786749]]
(2, 1); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.907675
actor_epsilon 2: 0.58606557377


### EPISODE 928###
Exploring

New Goal: 6
State-Actions: 
Here ------> [[ 0.23457026  0.34186435]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.06924631  0.34154296]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.31659836  0.34233901]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.38306695  0.3486934 ]]
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.9076
actor_epsilon 6: 0.825870646766


### EPISODE 929###
Exploring

New Goal: 3
State-Actions: 
(2, 0); 
next_state, external_reward, done 1 0.01 True
meta_epsilon: 0.907525
actor_epsilon 3: 0.622340425532


### EPISODE 930###
Exploring

New Goal: 5
State-Actions: 
Here ------> [[ 0.05343509  0.32573169]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
Here ------> [[ 0.05027285  0.32256946]]
(3, 1); 
next_state, external_reward, done 2 0.0 False
Here ------> [[ 0.0471106   0.31940722]]
(2, 1); 
next_state, external_reward, done 3 0.0 False
